{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Song Lyrics\n",
    "\n",
    "We will perform topic modeling using two techniques: Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) using tools from scikit-learn and gensim. All topic modeling code is contained in the `modeling.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/lyrics_drake.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform TFIDF Vectorization\n",
    "\n",
    "Before we can start topic modelling, we must apply term frequency-inverse document frequency (TFIDF) vectorization to our tokenized dataset. TFIDF is used to determine how important a word is to a document in a collection or corpus ([ref](https://www.wikiwand.com/en/Tf%E2%80%93idf)). For example, let's say the word \"like\" is very popular across all songs. Using TFIDF, we downweight the importance of \"like\" because it is a word that occurs frequently within our corpus. Let's say \"democracy\" is another word within that song but it is very rare across all songs. Its importance would be upweighted using TFDIF because it doesn't occur very often in our corpus.\n",
    "\n",
    "Note: scikit-learn's `TfidfVectorizer` expects an array of strings. So, we will need to concatenate our tokenized words together as a string for TFIDF to work properly. That being said, our concatenated tokenized words are very different from our original lyrics because we filtered out stopwords and performed lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF matrix dimensions: (373, 7161)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_hiphop = tfidf.fit_transform(data['processed_lyrics'])\n",
    "print(\"TFIDF matrix dimensions:\",X_hiphop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<373x7161 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50454 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hiphop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our TFIDF matrix, we can start topic modeling with NMF and LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=6, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 6\n",
    "nmf = NMF(n_components=n_topics)\n",
    "nmf.fit(X_hiphop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: know get like got say girl time one love wan let feel take want could baby never need thing tell\n",
      "Topic 1: cake million made rule cash bill like dog know le everything nigga sotto worldwide around hov doin got play share\n",
      "Topic 2: camera mean team knew thought ooh lie calling know care good girl wait knight taking one shining long scene stay\n",
      "Topic 3: home goin hold going endlessly emotion alone thing hard know want love good girl exactly something act different mark baby\n",
      "Topic 4: hell yeah right fuckin say fucked learned girl wit text like flew interview confession told confusing damn wish texting getting\n",
      "Topic 5: nigga shit like got fuck boy man bitch back real yeah new made money quick whole friend admit tell make\n"
     ]
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 20\n",
    "\n",
    "for i in range(0,n_topics):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':nmf.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    print(\"Topic %s:\" % i, ' '.join(sorted_topic['word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=20, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 20\n",
    "lda = LDA(n_components=n_components)\n",
    "lda.fit(X_hiphop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: versace medusa mazi truey handcuff metropolis ferragamo egyptian pissing remixing pharaoh dyckman overload exquisite optimist strictly gated binoculars lingo nicely\n",
      "Topic 1: get pleasure really count chainz killing ballplayer team whiskey nigga balled man carnival bitch agreed weh ashore unforgettable collect wildfire\n",
      "Topic 2: nobody like ready nigga much belong low shit prettiest versace doubt grammy bottom know beating chillin yeah make rather liable\n",
      "Topic 3: jumpman little tattoo frank home happening bit heartbreak smokin calabasas baddest nobu lightin babylon reside endlessly tiptoeing ooohhhh girrrrrrrrl shopping\n"
     ]
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 20\n",
    "k_topics = 4\n",
    "for i in range(0,k_topics):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':lda.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    print(\"Topic %s:\" % i, ' '.join(sorted_topic['word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NMF vs. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) are both topic modelling tools. The main difference is that LDA takes a Bayesian approach and adds a Dirichlet prior on top of the generative model. NMF’s topic-word probability distributions are fixed, while LDA’s topic-word distributions vary based on how the prior was tuned (hyperparameter $k$ - number of components). NMF would be a better choice if the topic probabilities are fixed for each document ref. Also, if our dataset is small, LDA may have inferior performance since it could introduce too much variability to the model ref.\n",
    "\n",
    "Unlike NMF, reconstructing X with LDA is not a closed-form solution. We need to use Monte Carlo simulations to sample from the distribution of Z (the distribution of topics for each sample), followed by the distribution of W (the distribution of words for topic $Z_i$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
