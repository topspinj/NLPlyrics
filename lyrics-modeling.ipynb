{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Song Lyrics\n",
    "\n",
    "We will perform topic modeling using two techniques: Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) using tools from scikit-learn and gensim. All topic modeling code is contained in the `modeling.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/lyrics_hiphop.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform TFIDF Vectorization\n",
    "\n",
    "Before we can start topic modelling, we must apply term frequency-inverse document frequency (TFIDF) vectorization to our tokenized dataset. TFIDF is used to determine how important a word is to a document in a collection or corpus ([ref](https://www.wikiwand.com/en/Tf%E2%80%93idf)). For example, let's say the word \"like\" is very popular across all songs. Using TFIDF, we downweight the importance of \"like\" because it is a word that occurs frequently within our corpus. Let's say \"democracy\" is another word within that song but it is very rare across all songs. Its importance would be upweighted using TFDIF because it doesn't occur very often in our corpus.\n",
    "\n",
    "Note: scikit-learn's `TfidfVectorizer` expects an array of strings. So, we will need to concatenate our tokenized words together as a string for TFIDF to work properly. That being said, our concatenated tokenized words are very different from our original lyrics because we filtered out stopwords and performed lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF matrix dimensions: (24846, 138554)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_hiphop = tfidf.fit_transform(data['processed_lyrics'])\n",
    "print(\"TFIDF matrix dimensions:\",X_hiphop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24846x138554 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3648296 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hiphop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our TFIDF matrix, we can start topic modeling with NMF and LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=10, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 10\n",
    "nmf = NMF(n_components=n_topics)\n",
    "nmf.fit(X_hiphop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: know life never time one day could would see say way feel world thing cause think take mind tell still\n",
      "Topic 1: nigga shit money fuck real got know thug wit get die gon hood motherfucker gun street ride yeah dog young\n",
      "Topic 2: ich und der die ist nicht da wir wie auf mich sie ein mir mit doch den wenn denn dich\n",
      "Topic 3: que con como por los soy la una pero esta todo para ma quiero cuando porque tengo sin vida hay\n",
      "Topic 4: girl baby yeah know wan want let got right tonight gon need tell get night make take body come like\n",
      "Topic 5: love baby heart need away like never ooh feel way make loving babe give hate hurt one chorus fall show\n",
      "Topic 6: instrumental harmony cell talk one dflo lyric pillow cooly redman verse shuttle groove slide version sex singing chorus waterfall peanut\n",
      "Topic 7: bitch fuck shit hoe as dick pussy fuckin fucking niggaz give money got like fucked motherfucker bad wan damn hook\n",
      "Topic 8: like get got back niggaz wit man money chorus shit come hit cause rock rap yeah make big boy rhyme\n",
      "Topic 9: dem yuh nuh gal man inna dat mek pon seh weh nah waan wid deh bwoy dance haffi tek bun\n"
     ]
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 20\n",
    "\n",
    "for i in range(0,n_components):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':nmf.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    print(\"Topic %s:\" % i, ' '.join(sorted_topic['word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 10\n",
    "lda = LDA(n_components=n_topics)\n",
    "lda.fit(X_hiphop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: like nigga got get know love shit bitch yeah let girl baby see one fuck want back make time man\n",
      "Topic 1: meg jireh egy juiciest hogy nem engedj chaaaange swaggggg swagk askjfas loveeeeeeeee juiccy pussssy onez vagy gyere nzz kpekkel csak\n",
      "Topic 2: surender oohoh kokoro rnrn lilijo najsliczniejsza matko boska maryjo flyover datz nani qurl tatoe mou dhat lng datte itsu rabbana\n",
      "Topic 3: ich und der ist nicht die da wir wie auf mich sie mit mir ein doch den wenn dich denn\n",
      "Topic 4: instermentual starbound anata koishikute somedayz jak dirtv slavic dake luuden kare scrubb hitburn kisi zobacz audrey zutto hitotsu eldo chikyuu\n",
      "Topic 5: shoobeedoo heythere cutle gotio shoobee snoobedeebeebop delacratic naya zindagi blessingis aroundyoull promiseto comesyou failyour struggleor comesyoull jabberwock borogoves raths outgrabe\n",
      "Topic 6: liberian naku hito watashi hsubakcits chiva breakadawn betsu anata dirtyface piya penda zoosk frenchie ayoooooo atashi ooooooooooooh eopsin koto naka\n",
      "Topic 7: que con como por los una pa soy la dans le pero para esta ma quiero todo cuando del vida\n",
      "Topic 8: ang mga hindi lang ako aking kung kahit nang sana isang ikaw kong bakit akin wala lahat pang wag ano\n",
      "Topic 9: instrumental che non per sono fokken jou poi fok questo tutto senza anche gli questa nel della ogni mun sam\n"
     ]
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 20\n",
    "\n",
    "for i in range(0,n_components):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':lda.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    print(\"Topic %s:\" % i, ' '.join(sorted_topic['word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NMF vs. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) are both topic modelling tools. The main difference is that LDA takes a Bayesian approach and adds a Dirichlet prior on top of the generative model. NMF’s topic-word probability distributions are fixed, while LDA’s topic-word distributions vary based on how the prior was tuned (hyperparameter $k$ - number of components). NMF would be a better choice if the topic probabilities are fixed for each document ref. Also, if our dataset is small, LDA may have inferior performance since it could introduce too much variability to the model ref.\n",
    "\n",
    "Unlike NMF, reconstructing X with LDA is not a closed-form solution. We need to use Monte Carlo simulations to sample from the distribution of Z (the distribution of topics for each sample), followed by the distribution of W (the distribution of words for topic $Z_i$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
