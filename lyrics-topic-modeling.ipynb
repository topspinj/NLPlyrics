{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Song Lyrics\n",
    "\n",
    "We will perform topic modeling using two techniques: Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) using tools from scikit-learn and gensim. All topic modeling code is contained in the `topic_modeling.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/lyrics_bob-dylan.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform TFIDF Vectorization\n",
    "\n",
    "Before we can start topic modelling, we must apply term frequency-inverse document frequency (TFIDF) vectorization to our tokenized dataset. TFIDF is used to determine how important a word is to a document in a collection or corpus ([ref](https://www.wikiwand.com/en/Tf%E2%80%93idf)). For example, let's say the word \"like\" is very popular across all songs. Using TFIDF, we downweight the importance of \"like\" because it is a word that occurs frequently within our corpus. Let's say \"democracy\" is another word within that song but it is very rare across all songs. Its importance would be upweighted using TFDIF because it doesn't occur very often in our corpus.\n",
    "\n",
    "Note: scikit-learn's `TfidfVectorizer` expects an array of strings. So, we will need to concatenate our tokenized words together as a string for TFIDF to work properly. That being said, our concatenated tokenized words are very different from our original lyrics because we filtered out stopwords and performed lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF matrix dimensions: (590, 2463)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=3, max_df=0.9)\n",
    "X = tfidf.fit_transform(data['processed_lyrics'])\n",
    "print(\"TFIDF matrix dimensions:\",X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scikit-learn's '`TfidfVectorizer`, you can specify a minimum and maximum document frequency (`min_df`, `max_df`). I set `min_df` to be 3, which means that a word must be mentioned in at least 3 documents in order for the vectorizer to include it. I set `max_df` to be 0.9 which will ignore words that appear in more than 90% of documents. You can think of it as a filter for corpus-specific stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<590x2463 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30381 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our TFIDF matrix, we can start topic modeling with NMF and LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization\n",
    "\n",
    "NMF was first published in the context of machine learning of facial images by Lee and Seung in 1999. It starts with a document-word matrix, $X_{ij}$, which represents the number of occurences of word $w_i$ in document $d_j$. We create our document-word matrix $X$ using tf-idf or count vectorization. This matrix gets factorized into two smaller matrices: a word-topic matrix $W_{ik}$ and topic-document matrix $H_{kj}$. $W_{ik}$ represents the $k$ topics discovered from the documents, while $H_{kj}$ represents the coefficient weights for the topics in each document. By reducing the dimensionality of our original document-word matrix, we are able to extract information about $k$ topics. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/matrix_factorization.png\" width=\"50%\"/>\n",
    "\n",
    "The process of factorizing $W$ and $H$ involves optimizing over an objective function, which in this case is the reconstruction error between $X$ and the product of its factors $W$ and $H$. $W$ and $H$ are updated iteratively until convergence (i.e., reconstruction error can no longer be minimized). In our example, a song represents one \"document\" in our $X$ matrix. Our goal is to reduce the dimensionality of our song-word matrix, $X$, so that we can extract meaningful $k$ topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0,\n",
       "  max_iter=200, n_components=6, random_state=1234, shuffle=False,\n",
       "  solver='cd', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "k_topics = 6\n",
    "nmf = NMF(n_components=k_topics, init=\"nndsvd\", random_state=1234)\n",
    "nmf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>time</td>\n",
       "      <td>instrumental</td>\n",
       "      <td>baby</td>\n",
       "      <td>said</td>\n",
       "      <td>love</td>\n",
       "      <td>knock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>lord</td>\n",
       "      <td>want</td>\n",
       "      <td>went</td>\n",
       "      <td>true</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>plow</td>\n",
       "      <td>lord</td>\n",
       "      <td>asked</td>\n",
       "      <td>know</td>\n",
       "      <td>knockin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>come</td>\n",
       "      <td>home</td>\n",
       "      <td>come</td>\n",
       "      <td>took</td>\n",
       "      <td>like</td>\n",
       "      <td>door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell</td>\n",
       "      <td>fixin</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>make</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>long</td>\n",
       "      <td>hand</td>\n",
       "      <td>night</td>\n",
       "      <td>clothes</td>\n",
       "      <td>heart</td>\n",
       "      <td>trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart</td>\n",
       "      <td>hold</td>\n",
       "      <td>like</td>\n",
       "      <td>dream</td>\n",
       "      <td>want</td>\n",
       "      <td>lord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eye</td>\n",
       "      <td>ground</td>\n",
       "      <td>mind</td>\n",
       "      <td>highway</td>\n",
       "      <td>blue</td>\n",
       "      <td>anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gone</td>\n",
       "      <td>believe</td>\n",
       "      <td>honey</td>\n",
       "      <td>little</td>\n",
       "      <td>seen</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>night</td>\n",
       "      <td>jackson</td>\n",
       "      <td>worry</td>\n",
       "      <td>shelter</td>\n",
       "      <td>need</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1             2      3        4      5        6\n",
       "0   time  instrumental   baby     said   love    knock\n",
       "1   know          lord   want     went   true   heaven\n",
       "2   like          plow   lord    asked   know  knockin\n",
       "3   come          home   come     took   like     door\n",
       "4   tell         fixin  right    right   make    close\n",
       "5   long          hand  night  clothes  heart   trying\n",
       "6  heart          hold   like    dream   want     lord\n",
       "7    eye        ground   mind  highway   blue  anymore\n",
       "8   gone       believe  honey   little   seen    lucky\n",
       "9  night       jackson  worry  shelter   need  walking"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 10\n",
    "word_dict = dict()\n",
    "\n",
    "for i in range(0,k_topics):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':nmf.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    word_dict[i+1] = list(sorted_topic['word'])\n",
    "\n",
    "pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We looked at the top 10 most \"relevant\" words across 6 topics in our corpus of Bob Dylan. Some of the topics are hard to summarize, but others are quite obvious. For example, Topic 5 is clearly about `love`, Topic 2 seems to have some religious undertones, and Topic 6 appears to represent lyrics from Bob Dylan's song `Knockin on Heaven's Door`.\n",
    "\n",
    "Note that results can change if you try out different $k$ topics. Choosing a small $k$ can result in extremely broad topics, while choosing a large $k$ can end up in over-clustering, which produces many highly-similar topics ([ref](https://arxiv.org/pdf/1404.4606.pdf)). There are strategies to identify optimal $k$ (e.g., term-centric stability analysis, k-clustering, etc.), but this is outside the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) was introduced in 2013 as a generative probabilistic model for topic discovery ([ref](http://jmlr.csail.mit.edu/papers/v3/blei03a.html)). The basic idea behind LDA is described as such:\n",
    "\n",
    "> documents are represented as random mixtures over latent topics, where each topic is characterized\n",
    "by a distribution over words\n",
    "\n",
    "In other words, LDA treats each document as a mixture of topics, and each topic as a mixture of words. We can think of LDA as a type of `soft clustering` in which a topic represetns a \"cluster\" and a document's topic probabilities represent their proporition of cluster membership. Unlike k-means clustering where each document strictly belongs to a single cluster (i.e., topic), LDA allows documents to belong to a mixture of topics.\n",
    "\n",
    "The main difference between LDA and NMF is that LDA is probabilistic in nature. Unlike NMF, LDA is a hierarchical Bayesian model with a **Dirichlet prior**. A Dirichlet distribution is a multivariate probability distribution parameterized by $\\alpha$. This parameter, $\\alpha$ is used to determine a document's topic mixture distribution. Assuming a symmetric Dirichlet prior, a large $\\alpha$ means that each document will contain a mixture of most (or all) topics, while a small $\\alpha$ means that a document will contain a mixture of a small number of topics. We can think of a very small $\\alpha$ as being a type of \"hard clustering\" in which a document is assigned to a single topic.  \n",
    "\n",
    "In scikit-learn's `LatentDirichletAllocation` class, $\\alpha$ is described as the `doc_topic_prior` which has a default of 1/k-topics. There is also a `topic_word_prior`, also known as $\\beta$, which determines a given word's topic distribution. Its default is also 1/k-topics. For the purpose of this analysis, let's keep the default priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=15, mean_change_tol=0.001,\n",
       "             n_components=6, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=1234, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "k_topics = 6\n",
    "lda = LDA(n_components=k_topics, max_iter=15, learning_method='online', random_state=1234)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>know</td>\n",
       "      <td>ramble</td>\n",
       "      <td>success</td>\n",
       "      <td>act</td>\n",
       "      <td>knock</td>\n",
       "      <td>instrumental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>santa</td>\n",
       "      <td>failure</td>\n",
       "      <td>evidently</td>\n",
       "      <td>jane</td>\n",
       "      <td>critic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>claus</td>\n",
       "      <td>speaks</td>\n",
       "      <td>supposed</td>\n",
       "      <td>knockin</td>\n",
       "      <td>congressman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>come</td>\n",
       "      <td>swing</td>\n",
       "      <td>ideal</td>\n",
       "      <td>tired</td>\n",
       "      <td>stone</td>\n",
       "      <td>rapidly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baby</td>\n",
       "      <td>beard</td>\n",
       "      <td>dangles</td>\n",
       "      <td>riding</td>\n",
       "      <td>heaven</td>\n",
       "      <td>prophesize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time</td>\n",
       "      <td>sleigh</td>\n",
       "      <td>horseman</td>\n",
       "      <td>horse</td>\n",
       "      <td>stoned</td>\n",
       "      <td>stalled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>said</td>\n",
       "      <td>reindeer</td>\n",
       "      <td>madam</td>\n",
       "      <td>dreamin</td>\n",
       "      <td>door</td>\n",
       "      <td>writer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>want</td>\n",
       "      <td>dawn</td>\n",
       "      <td>matchstick</td>\n",
       "      <td>whirling</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>rattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>night</td>\n",
       "      <td>cherry</td>\n",
       "      <td>raven</td>\n",
       "      <td>myth</td>\n",
       "      <td>libido</td>\n",
       "      <td>drenched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tell</td>\n",
       "      <td>break</td>\n",
       "      <td>banker</td>\n",
       "      <td>swirling</td>\n",
       "      <td>subjugation</td>\n",
       "      <td>senator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1         2           3          4            5             6\n",
       "0   know    ramble     success        act        knock  instrumental\n",
       "1   love     santa     failure  evidently         jane        critic\n",
       "2   like     claus      speaks   supposed      knockin   congressman\n",
       "3   come     swing       ideal      tired        stone       rapidly\n",
       "4   baby     beard     dangles     riding       heaven    prophesize\n",
       "5   time    sleigh    horseman      horse       stoned       stalled\n",
       "6   said  reindeer       madam    dreamin         door        writer\n",
       "7   want      dawn  matchstick   whirling    breakfast        rattle\n",
       "8  night    cherry       raven       myth       libido      drenched\n",
       "9   tell     break      banker   swirling  subjugation       senator"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 10\n",
    "word_dict = dict()\n",
    "for i in range(0,k_topics):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':lda.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    word_dict[i+1] = list(sorted_topic['word'])\n",
    "\n",
    "pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA's results are significantly different from what we saw with NMF. The topics seem to be much more coherent and interpretable. Here are my interpretations of what each topic represents:\n",
    "\n",
    "- Topic 1 - love\n",
    "- Topic 2 - Christmas\n",
    "- Topic 3 - unclear\n",
    "- Topic 4 - scenes from a dream\n",
    "- Topic 5 - `Knockin on Heaven's Door` (similar to NMF's Topic 6)\n",
    "- Topic 6 - politics?\n",
    "\n",
    "Some of these topics are open to interpretation and perhaps trying to optimize $$k$$ would improve topic interpretability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el410214410719832972749133\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el410214410719832972749133_data = {\"mdsDat\": {\"Freq\": [86.21232022049877, 2.877155866072146, 2.810666895328155, 2.79909459685541, 2.675049634307072, 2.6257127869384616], \"cluster\": [1, 1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5, 6], \"x\": [-55.687599182128906, -82.98976135253906, 105.06681823730469, 16.361940383911133, 60.537960052490234, 8.657819747924805], \"y\": [86.61988067626953, -26.715848922729492, -12.061854362487793, -87.70437622070312, 95.67646789550781, 11.162946701049805]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"Freq\": [2.0, 4.0, 3.0, 6.0, 1.0, 1.0, 2.0, 0.0, 3.0, 4.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 5.0, 2.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 18.573272582206684, 16.83345341869319, 16.76446356544065, 15.934875884236375, 14.405763594337904, 14.006115730134537, 13.754171373194755, 11.197153335050876, 11.037780923718282, 10.665284933582402, 9.739846127609216, 9.185471325811891, 9.137408947253824, 8.886248934330531, 8.625917534791078, 8.021348408966654, 7.692281022789103, 7.591667590144921, 7.608687926571136, 7.507575789551011, 7.215147773761668, 7.128725411857627, 7.185026198339143, 7.095555101638896, 6.923693842150068, 6.900559025811932, 6.794291010353463, 6.68362136569292, 6.656033081135755, 6.627046361008437, 6.68405956944298, 0.176294817875334, 0.17631206006201947, 0.10883761627284864, 0.10882054805182062, 0.1088152101129302, 0.10885530699680462, 0.10884397673395887, 0.10879959003151833, 0.10880248295143036, 0.10877041684369775, 0.10665154583948926, 0.10579443515516418, 0.10579640493712357, 0.10498435215751298, 0.10457777452724006, 0.10440898721341421, 0.10202910744774249, 0.10285353510330014, 0.09872193538149869, 0.09767695328061628, 0.09452536811261827, 0.09553221394870025, 0.09110821662149235, 0.09435660372924694, 0.08917205649229151, 0.08845781061019029, 0.08699410804410648, 0.08518998267757927, 0.08810110561655385, 0.13311827785442581, 0.0933271924304824, 0.5740773394265059, 0.2209027358381293, 0.37911432805334383, 0.5930799007886092, 0.07498906771407837, 0.049131616070974715, 0.04907854234791008, 0.049098182399603806, 0.04910290254471448, 0.04861968540973413, 0.04847977181834531, 0.32125113159547286, 0.0353968106442143, 0.03538890188717724, 0.03528942801835192, 0.03527143526358424, 0.03530203969759468, 0.035245887807167725, 0.03523986473370255, 0.035192354081147716, 0.03534326586291443, 0.03522544779725825, 0.03525775318324029, 0.03538415188005711, 0.035304972629983215, 0.03529198366343688, 0.03526577696986706, 0.03831103200222093, 0.03532714136104509, 0.03526847581328064, 0.24806383514641914, 0.1704350910533068, 0.04238543693891545, 0.04308822035769687, 0.03717281356950281, 0.041245349359963636, 0.04084156655502043, 0.04266657369539903, 0.039358726613778657, 0.037407562402259224, 0.0387146564910756, 0.038588419884536505, 0.03884041838160785, 0.03817130651670909, 0.03718474425251494, 0.040283452902194886, 0.037597560666185234, 0.03865679075610315, 0.037983148768053865, 0.03778110939045722, 0.03761008680976999, 2.0157712487256645, 0.11999885666702192, 0.1197274128790673, 0.11969314527034965, 0.11956365583221658, 0.11482517642012968, 0.11228636333187782, 0.10965272292142968, 0.10100894825165807, 0.08336612843953786, 0.03510075070094827, 0.03505494555023061, 0.035059188963202634, 0.0350323450910545, 0.03509847025153183, 0.03503024366827259, 0.035106613354108925, 0.03513545834180197, 0.03508584870604791, 0.03506569732752185, 0.03506170573664342, 0.03506085074083073, 0.03507060543974327, 0.03502357915543537, 0.03509410302283143, 0.03504859112199839, 0.03503867837775183, 0.035074518093621165, 0.035098921171956676, 0.03506743643749282, 0.04907642627932212, 0.040956049247794606, 0.04022832989193528, 0.04402348040305947, 0.0383158356920164, 0.03783725881583097, 0.03787744467569441, 0.03794850644686694, 0.037756910294305314, 0.03802892198990949, 0.03765727827460664, 0.037166238494820744, 0.036696870541838914, 0.03659348585598675, 0.03655017366549959, 0.036491694791263574, 0.03655482345411623, 0.036311098829043306, 0.03544640807544214, 0.036905108354074914, 0.036001236441432105, 0.03619162202180053, 0.2498041659438031, 0.17763930796614733, 0.1044345243255735, 0.10423776520530814, 0.10399428089243168, 0.06976929514937708, 0.06355991937270035, 0.0341349542120326, 0.034169090266974636, 0.03415437077042853, 0.034101130737797414, 0.03411241940761944, 0.12591216624746998, 0.12878388086729445, 0.03410696921457364, 0.12448813708138495, 0.03419240955753344, 0.03418069260395586, 0.03418225271236185, 0.034177524272778675, 0.03412092563564803, 0.034100795780313865, 0.0341265860281472, 0.03410428650481511, 0.034219788112873045, 0.034121790100711286, 0.0341930095456916, 0.034137960071855, 0.0341251707890576, 0.034193465918614115, 0.04657027364628609, 0.11368239012230595, 0.04171061784636755, 0.037902928530103644, 0.03769566851492734, 0.03732219339472872, 0.03689213988035442, 0.03644785079262528, 0.036084967299938174, 0.03603896206290781, 0.03602137835608563, 0.03612169635526624, 0.035807585009255294, 0.035781612018014594, 0.03662123639502693, 0.035766274135693873, 0.03510430766225505, 0.035051916918344576, 0.03498358267813936, 0.03460823631317387, 0.03498117389733343, 0.26463159942140796, 0.17439056553196544, 0.1393262346208541, 0.2233762537444375, 0.033716960076550416, 0.033627321601671374, 0.03362523398292216, 0.0335888969000139, 0.033686448215458534, 0.03362963047156691, 0.03368125149068553, 0.03364719415211176, 0.033620560374935084, 0.03363406383328452, 0.033632290276706535, 0.033605779771673526, 0.03360744157220515, 0.03357246772575826, 0.033635175054410146, 0.0336376615504495, 0.033590532691529514, 0.033604244238199826, 0.03364759332296378, 0.03364124495174459, 0.033588147885063376, 0.033698677817209685, 0.03367385377059392, 0.049866113466293965, 0.03365890723620165, 0.033613870751902516, 0.04350113231265292, 0.041059865553093024, 0.0348691580517433, 0.0383439382652183, 0.04075567267384208, 0.035942342541855944, 0.034993632168570606, 0.038049778103200343, 0.036954378337778954, 0.0357557026272219, 0.03562543821066939, 0.034688721100863344, 0.03472317377061543, 0.034634240132495595, 0.034719216332675676, 0.034103191597260045, 0.03722593227559268, 0.03507935436597361, 0.03454269796351406, 0.03469828073800752, 0.03451694295846201, 0.03448578572719615, 0.03425617532914605, 0.03474085352714451, 0.0345857237209546, 0.03426390214693086, 0.03427957435157702, 0.034601135266010524, 0.03454146075639792, 0.0345408386619707], \"Term\": [\"instrumental\", \"heaven\", \"stone\", \"door\", \"knock\", \"santa\", \"horse\", \"jane\", \"father\", \"dark\", \"break\", \"cold\", \"dear\", \"black\", \"king\", \"mama\", \"day\", \"window\", \"trouble\", \"dawn\", \"blow\", \"child\", \"ground\", \"somebody\", \"rock\", \"yeah\", \"young\", \"soon\", \"sure\", \"mile\", \"know\", \"love\", \"like\", \"come\", \"baby\", \"time\", \"said\", \"want\", \"night\", \"tell\", \"heart\", \"away\", \"gone\", \"right\", \"make\", \"long\", \"mind\", \"thing\", \"lord\", \"hand\", \"hear\", \"babe\", \"good\", \"friend\", \"world\", \"little\", \"eye\", \"dream\", \"went\", \"look\", \"home\", \"failure\", \"success\", \"horseman\", \"madam\", \"matchstick\", \"ideal\", \"dangles\", \"banker\", \"raven\", \"quotation\", \"conclusion\", \"ceremony\", \"statue\", \"perfection\", \"valentine\", \"crumble\", \"cloak\", \"rainy\", \"niece\", \"dagger\", \"wink\", \"argue\", \"hammer\", \"grudge\", \"violence\", \"candle\", \"expecting\", \"repeat\", \"tremble\", \"speaks\", \"ramble\", \"jane\", \"stoned\", \"knockin\", \"knock\", \"breakfast\", \"libido\", \"mortician\", \"tattered\", \"subjugation\", \"sniffin\", \"conclusion\", \"stone\", \"banker\", \"ideal\", \"dangles\", \"horseman\", \"quotation\", \"madam\", \"raven\", \"matchstick\", \"whirling\", \"dreamin\", \"myth\", \"congressman\", \"critic\", \"rapidly\", \"prophesize\", \"boiler\", \"spiritual\", \"kicked\", \"heaven\", \"door\", \"buck\", \"badge\", \"yankee\", \"rank\", \"disease\", \"anymore\", \"shoot\", \"creeping\", \"gun\", \"bank\", \"luck\", \"gettin\", \"able\", \"feel\", \"jack\", \"mama\", \"dark\", \"dear\", \"like\", \"instrumental\", \"critic\", \"congressman\", \"rapidly\", \"prophesize\", \"stalled\", \"writer\", \"rattle\", \"drenched\", \"senator\", \"tattered\", \"mortician\", \"subjugation\", \"libido\", \"breakfast\", \"sniffin\", \"matchstick\", \"quotation\", \"ideal\", \"horseman\", \"dangles\", \"banker\", \"raven\", \"madam\", \"whirling\", \"dreamin\", \"myth\", \"conclusion\", \"spiritual\", \"kicked\", \"admit\", \"ragin\", \"son\", \"father\", \"accept\", \"criticize\", \"loser\", \"heed\", \"grown\", \"battle\", \"command\", \"lend\", \"spin\", \"sink\", \"roam\", \"hall\", \"doorway\", \"tellin\", \"wheat\", \"time\", \"speak\", \"soon\", \"act\", \"evidently\", \"dreamin\", \"whirling\", \"myth\", \"swirling\", \"swayed\", \"mortician\", \"subjugation\", \"libido\", \"tattered\", \"breakfast\", \"tired\", \"supposed\", \"sniffin\", \"riding\", \"madam\", \"horseman\", \"dangles\", \"banker\", \"ideal\", \"matchstick\", \"quotation\", \"raven\", \"congressman\", \"critic\", \"prophesize\", \"rapidly\", \"conclusion\", \"spiritual\", \"facing\", \"horse\", \"skirt\", \"whispering\", \"gladly\", \"chase\", \"asks\", \"acting\", \"confess\", \"unlock\", \"watery\", \"turnin\", \"deserted\", \"blazing\", \"pretend\", \"nighttime\", \"mystery\", \"easily\", \"doubt\", \"waking\", \"like\", \"ramble\", \"claus\", \"swing\", \"santa\", \"subjugation\", \"mortician\", \"tattered\", \"libido\", \"breakfast\", \"sniffin\", \"banker\", \"matchstick\", \"madam\", \"ideal\", \"dangles\", \"horseman\", \"quotation\", \"raven\", \"dreamin\", \"whirling\", \"myth\", \"critic\", \"congressman\", \"rapidly\", \"prophesize\", \"conclusion\", \"spiritual\", \"beard\", \"kicked\", \"wandering\", \"sleigh\", \"reindeer\", \"cupid\", \"cherry\", \"dawn\", \"drank\", \"happens\", \"break\", \"glory\", \"special\", \"danced\", \"cat\", \"afford\", \"headed\", \"lingered\", \"dancer\", \"trouble\", \"adore\", \"foggy\", \"newborn\", \"reward\", \"compared\", \"joyful\", \"suit\", \"lane\", \"nation\", \"bethlehem\", \"seven\", \"tonight\", \"tomorrow\"], \"Total\": [2.0, 4.0, 3.0, 6.0, 1.0, 1.0, 2.0, 0.0, 3.0, 4.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 5.0, 2.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 18.74990545604879, 17.01165447461498, 16.94351596706095, 16.11217888415498, 14.582221500787748, 14.183502776626288, 13.93002100642699, 11.373032496348223, 11.214696736209115, 10.841088956725565, 9.91609895208071, 9.36183949891112, 9.313789329805159, 9.06314912757219, 8.801975481219703, 8.197417755560737, 7.867221841442655, 7.767347579008198, 7.785646262549083, 7.6837169054554675, 7.391165929620029, 7.303183288358531, 7.361249533982364, 7.270812650702512, 7.09951052023893, 7.076763435095504, 6.969512432397738, 6.859542035649262, 6.831256084045979, 6.802349557430655, 6.860885270674751, 0.5219238961241297, 0.5338772965857655, 0.37816953095462, 0.3781107162669875, 0.37813357968348893, 0.37831504737450206, 0.3783336005058806, 0.3783298373153981, 0.37847992632318567, 0.37855239848843625, 0.4030284037356829, 0.42217920415028054, 0.4435725585266643, 0.4548896529479027, 0.48063845886343637, 0.48773269119094936, 0.47919263643080373, 0.5205682243588509, 0.5432179230120986, 0.5707001515718941, 0.5663208604251686, 0.6118772487778936, 0.6026811907008708, 0.6310863450912496, 0.6473475854477861, 0.6632234059212265, 0.6745879034091266, 0.6656157656676093, 0.706066990588037, 1.1039299710613253, 0.8326041825045172, 0.8489550060653169, 0.4920458895620689, 0.8966386934623587, 1.840080880629774, 0.3471884618447339, 0.31924505883036236, 0.3189225569412049, 0.319154303024668, 0.319362071143018, 0.35725335601138253, 0.4030284037356829, 3.370236231120329, 0.3783298373153981, 0.37831504737450206, 0.3783336005058806, 0.37816953095462, 0.37855239848843625, 0.3781107162669875, 0.37847992632318567, 0.37813357968348893, 0.3833765198429476, 0.382960503299067, 0.3839172969164077, 0.39388682918185075, 0.39327980107963234, 0.394097629401912, 0.39436487752398197, 0.43931472507862646, 0.40948693779205975, 0.41720454031030363, 4.680389645462605, 6.945818956240329, 0.6273742772288382, 0.7115027169173075, 0.4987104831616863, 0.94096662820466, 1.0350957486156838, 1.9927420648655068, 1.0736490101638474, 0.7422421269803369, 1.188863052944237, 1.163391991945861, 1.5311456811217727, 1.3663238849271482, 0.8922629809264447, 6.536921473604811, 1.7534077320630124, 4.99059533508443, 4.056289253602421, 4.498093508344918, 16.94351596706095, 2.2857458008873173, 0.39327980107963234, 0.39388682918185075, 0.394097629401912, 0.39436487752398197, 0.4329684932451859, 0.4446598699626232, 0.4777593818839001, 0.5095533511486531, 0.5821548794135473, 0.319154303024668, 0.3189225569412049, 0.319362071143018, 0.31924505883036236, 0.3471884618447339, 0.35725335601138253, 0.37813357968348893, 0.37855239848843625, 0.37831504737450206, 0.37816953095462, 0.3783336005058806, 0.3783298373153981, 0.37847992632318567, 0.3781107162669875, 0.3833765198429476, 0.382960503299067, 0.3839172969164077, 0.4030284037356829, 0.40948693779205975, 0.41720454031030363, 0.7279540342065705, 0.6076321356533061, 0.8202876840213121, 3.2175123291797583, 1.0115373655170796, 0.9552385358437445, 0.9733454830982986, 1.062140536086164, 1.0245836525076406, 1.1415094012749145, 1.0154434450342376, 1.0258122361948538, 1.2606211602068726, 1.2433991296401234, 1.2094801372576978, 1.1489095589207765, 1.2788223175969144, 1.2193529958572438, 0.5272816128253871, 14.183502776626288, 2.069183131981277, 4.622615077566507, 0.5245593680746024, 0.45250874288963516, 0.382960503299067, 0.3833765198429476, 0.3839172969164077, 0.5226926481073746, 0.5459668473590179, 0.3189225569412049, 0.319362071143018, 0.31924505883036236, 0.319154303024668, 0.3471884618447339, 1.3158857701783415, 1.3467279626360067, 0.35725335601138253, 1.3577283237891618, 0.3781107162669875, 0.37816953095462, 0.3783336005058806, 0.3783298373153981, 0.37831504737450206, 0.37813357968348893, 0.37855239848843625, 0.37847992632318567, 0.39388682918185075, 0.39327980107963234, 0.39436487752398197, 0.394097629401912, 0.4030284037356829, 0.40948693779205975, 0.6055906244478381, 2.0426141195703353, 0.6661658446699469, 0.6838936224270755, 0.7481971969368832, 0.7257020044033925, 0.7276967640617517, 0.7518378929500715, 0.7475877295720871, 0.7762722600731159, 0.8250791796162986, 0.9027377341015542, 0.8078688346114912, 0.8248368374734503, 1.3123935741236272, 0.8314399763248694, 0.9731648660506876, 0.9742144702535231, 1.2470156475014147, 0.9002628898198402, 16.94351596706095, 0.8326041825045172, 0.6080798604996479, 0.5844679442239163, 1.42009121971157, 0.319362071143018, 0.3189225569412049, 0.319154303024668, 0.31924505883036236, 0.3471884618447339, 0.35725335601138253, 0.3783298373153981, 0.37813357968348893, 0.3781107162669875, 0.37831504737450206, 0.3783336005058806, 0.37816953095462, 0.37855239848843625, 0.37847992632318567, 0.382960503299067, 0.3833765198429476, 0.3839172969164077, 0.39327980107963234, 0.39388682918185075, 0.394097629401912, 0.39436487752398197, 0.4030284037356829, 0.40948693779205975, 0.6138305199000028, 0.41720454031030363, 0.42212543170042893, 0.5685180757824212, 0.6053512542963516, 0.4516926707370831, 0.6781892114034299, 2.8527105440336094, 0.9155942364767009, 0.6943991569447346, 3.2492223368897744, 1.9407206447111782, 1.0980886399419492, 1.0997410738966809, 0.6688304172516183, 0.7079892044728032, 0.6784107142202515, 0.7406741802514166, 0.5236732076831929, 5.1187992962723525, 1.1470952642378158, 0.785704892230146, 0.9431279629399028, 0.859124220607421, 0.8258847260679548, 0.6162767373039698, 1.2918393371585628, 1.0331376129905496, 0.649076410556397, 0.6737958831967479, 2.121309297476731, 4.739223279535815, 2.7965121824424637], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.1389, 0.1378, 0.1377, 0.1373, 0.1362, 0.1358, 0.1357, 0.1328, 0.1325, 0.132, 0.1304, 0.1293, 0.1292, 0.1286, 0.1282, 0.1266, 0.1259, 0.1255, 0.1254, 0.1252, 0.1243, 0.1242, 0.1241, 0.124, 0.1233, 0.1231, 0.1229, 0.1224, 0.1224, 0.1222, 0.1222, 2.463, 2.4405, 2.3029, 2.3029, 2.3028, 2.3027, 2.3025, 2.3021, 2.3017, 2.3013, 2.2189, 2.1644, 2.115, 2.0821, 2.0232, 2.0069, 2.0015, 1.9268, 1.8432, 1.7832, 1.7581, 1.6913, 1.659, 1.648, 1.5661, 1.5338, 1.5001, 1.4925, 1.4671, 1.433, 1.3599, 3.1805, 2.7709, 2.7109, 2.4395, 2.0392, 1.7003, 1.7002, 1.6999, 1.6993, 1.5773, 1.4539, 1.2212, 1.2026, 1.2024, 1.1996, 1.1995, 1.1993, 1.1989, 1.1978, 1.1973, 1.1878, 1.1856, 1.184, 1.1619, 1.1613, 1.1588, 1.1574, 1.1323, 1.1215, 1.1012, 0.6343, -0.1358, 0.877, 0.7676, 0.9753, 0.4444, 0.3392, -0.2721, 0.2656, 0.5839, 0.1472, 0.1656, -0.1026, -0.006, 0.3939, -1.5175, -0.2706, -1.2888, -1.0991, -1.2079, -2.5386, 3.4502, 2.3888, 2.385, 2.3842, 2.3824, 2.2486, 2.1996, 2.1041, 1.9575, 1.6324, 1.3684, 1.3678, 1.3666, 1.3662, 1.2842, 1.2536, 1.199, 1.1987, 1.1979, 1.1978, 1.1972, 1.1972, 1.1971, 1.1967, 1.1849, 1.1847, 1.1819, 1.1343, 1.1191, 1.0996, 0.879, 0.8788, 0.5608, -0.7158, 0.3025, 0.3472, 0.3295, 0.2441, 0.275, 0.1741, 0.2813, 0.258, 0.0392, 0.0501, 0.0766, 0.1264, 0.021, 0.0619, 0.8762, -2.3756, -0.4755, -1.274, 2.8793, 2.6862, 2.3218, 2.3189, 2.3151, 1.6074, 1.4706, 1.3866, 1.3862, 1.3861, 1.3849, 1.301, 1.2745, 1.2739, 1.2723, 1.2318, 1.218, 1.2175, 1.2171, 1.217, 1.2154, 1.2153, 1.2149, 1.2145, 1.1779, 1.1766, 1.1759, 1.175, 1.1522, 1.1383, 1.056, 0.7326, 0.8504, 0.7284, 0.6331, 0.6537, 0.6393, 0.5946, 0.5902, 0.5513, 0.4898, 0.4027, 0.505, 0.4835, 0.0422, 0.475, 0.299, 0.2964, 0.0476, 0.3626, -2.5616, 2.4936, 2.3908, 2.2059, 1.7902, 1.3915, 1.3902, 1.3894, 1.3881, 1.307, 1.2768, 1.221, 1.2205, 1.2198, 1.2196, 1.2195, 1.2192, 1.2182, 1.2174, 1.2075, 1.2064, 1.2036, 1.1799, 1.1797, 1.179, 1.1767, 1.1583, 1.1416, 1.1294, 1.1225, 1.1095, 1.0696, 0.949, 1.0784, 0.767, -0.6086, 0.4022, 0.6519, -0.8075, -0.3213, 0.2152, 0.21, 0.6807, 0.6248, 0.6649, 0.5796, 0.9083, -1.2839, 0.1524, 0.5154, 0.3373, 0.4254, 0.4639, 0.75, 0.0239, 0.2429, 0.6984, 0.6614, -0.4761, -1.2817, -0.7542], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.9968, -5.0951, -5.0992, -5.15, -5.2509, -5.279, -5.2972, -5.5028, -5.5172, -5.5515, -5.6423, -5.7009, -5.7061, -5.734, -5.7637, -5.8364, -5.8783, -5.8914, -5.8892, -5.9026, -5.9423, -5.9544, -5.9465, -5.959, -5.9835, -5.9869, -6.0024, -6.0188, -6.023, -6.0273, -6.0188, -6.2541, -6.254, -6.7364, -6.7365, -6.7366, -6.7362, -6.7363, -6.7367, -6.7367, -6.737, -6.7567, -6.7647, -6.7647, -6.7724, -6.7763, -6.7779, -6.801, -6.7929, -6.8339, -6.8446, -6.8774, -6.8668, -6.9142, -6.8792, -6.9357, -6.9437, -6.9604, -6.9814, -6.9478, -6.535, -6.8901, -5.0501, -6.0051, -5.465, -5.0175, -7.0855, -7.5084, -7.5094, -7.509, -7.5089, -7.5188, -7.5217, -5.6306, -7.8362, -7.8365, -7.8393, -7.8398, -7.8389, -7.8405, -7.8407, -7.842, -7.8378, -7.8411, -7.8402, -7.8366, -7.8388, -7.8392, -7.8399, -7.7571, -7.8382, -7.8399, -5.8892, -6.2645, -7.6561, -7.6396, -7.7873, -7.6833, -7.6932, -7.6494, -7.7301, -7.781, -7.7466, -7.7499, -7.7434, -7.7608, -7.787, -7.7069, -7.7759, -7.7481, -7.7657, -7.7711, -7.7756, -3.79, -6.6113, -6.6135, -6.6138, -6.6149, -6.6553, -6.6777, -6.7014, -6.7835, -6.9755, -7.8405, -7.8418, -7.8417, -7.8425, -7.8406, -7.8425, -7.8403, -7.8395, -7.8409, -7.8415, -7.8416, -7.8417, -7.8414, -7.8427, -7.8407, -7.842, -7.8423, -7.8413, -7.8406, -7.8415, -7.5054, -7.6862, -7.7042, -7.614, -7.7529, -7.7654, -7.7644, -7.7625, -7.7676, -7.7604, -7.7702, -7.7833, -7.796, -7.7989, -7.8001, -7.8017, -7.7999, -7.8066, -7.8307, -7.7904, -7.8152, -7.8099, -5.8327, -6.1737, -6.7048, -6.7067, -6.7091, -7.1082, -7.2014, -7.8231, -7.8221, -7.8225, -7.8241, -7.8237, -6.5178, -6.4953, -7.8239, -6.5292, -7.8214, -7.8217, -7.8217, -7.8218, -7.8235, -7.8241, -7.8233, -7.824, -7.8206, -7.8235, -7.8214, -7.823, -7.8234, -7.8214, -7.5124, -6.62, -7.6227, -7.7184, -7.7239, -7.7338, -7.7454, -7.7575, -7.7675, -7.7688, -7.7693, -7.7665, -7.7752, -7.776, -7.7528, -7.7764, -7.7951, -7.7966, -7.7985, -7.8093, -7.7986, -5.7565, -6.1735, -6.398, -5.9259, -7.8168, -7.8195, -7.8195, -7.8206, -7.8177, -7.8194, -7.8179, -7.8189, -7.8197, -7.8193, -7.8193, -7.8201, -7.82, -7.8211, -7.8192, -7.8191, -7.8205, -7.8201, -7.8189, -7.819, -7.8206, -7.8173, -7.8181, -7.4255, -7.8185, -7.8199, -7.562, -7.6198, -7.7832, -7.6882, -7.6272, -7.7529, -7.7796, -7.6959, -7.7251, -7.7581, -7.7617, -7.7884, -7.7874, -7.79, -7.7875, -7.8054, -7.7178, -7.7772, -7.7926, -7.7881, -7.7933, -7.7942, -7.8009, -7.7869, -7.7914, -7.8007, -7.8002, -7.7909, -7.7926, -7.7927]}, \"token.table\": {\"Topic\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [1.1207458130356265, 0.9885942270543986, 1.3300739552727074, 1.3737131096332813, 0.8717671767779873, 1.4124509154693108, 1.0036421849382615, 1.3741987726018539, 0.9613495297635464, 0.9584861455083822, 0.9600731959285973, 1.4054760104538326, 0.859555512607083, 0.8760330829366212, 0.9681566312961413, 1.212360984098492, 0.8204840453444737, 0.9232978506702205, 1.3779760754858474, 0.9552642078979683, 0.9062712084679178, 0.9930376341423756, 0.9847914277158815, 1.2108227315947706, 1.3376356519018733, 1.347269258440368, 1.046858938868833, 0.9093049479881012, 0.9861229685352368, 1.0516314058832374, 1.041084775845319, 0.8892656394490579, 1.2378247026708313, 0.9660942007900041, 1.007800526345564, 0.7819694622464358, 0.8019145565684376, 1.092186866365718, 1.0204762888864554, 1.0264680217075473, 1.0043744189996047, 0.9323973595354618, 0.9178632517198155, 1.2727424888008505, 0.9627534549832822, 0.7318908869497803, 1.336546039057613, 1.0305450222577726, 0.9663091660446947, 0.9509255144368226, 0.8555908839538037, 0.9760062026683006, 0.8411397742771846, 0.8703905300773596, 1.0411627729699373, 1.4400939142839244, 1.4740333238241605, 0.9470765595922512, 1.00846109425942, 0.8546297003023653, 0.9414949962128901, 1.0202764984162995, 0.9791374596101885, 0.8749879357641642, 1.1406360103401927, 1.1779187269708637, 0.9747511895903275, 0.5434543723196269, 0.5434543723196269, 1.0133384429344163, 0.9679252670952241, 0.9748372701318111, 1.003333666580706, 1.350121317392969, 0.9891527481737181, 0.9759170800552627, 1.0290562019638403, 1.0275319132442495, 1.027384435808811, 0.9993149123365767, 0.6531057183712028, 1.0224977357870186, 1.0018844775591107, 1.0558496140922127, 1.016877388388605, 1.0275751158775543, 1.0603015065769195, 0.9808557697761082, 1.2027326427341147, 0.7619665470152631, 1.0627369452070516, 1.1639760304894868, 0.7365243712447495, 0.9930323194859416, 0.8268015068584257, 1.0413563791550755, 1.0050236100534753, 0.7041801161217704, 0.9428139509778104, 0.9314030847449782, 0.8042469840632989, 1.0374018242672831, 1.2190844986208749, 0.8653110702234218, 0.9665650029173432, 0.9058545616245871, 0.9106732950564586, 0.7932597290655475, 0.8901453174998194, 0.7740900677320512, 0.7425404593535418, 1.0621820863714182, 1.0146582178145351, 0.8201070595615081, 1.0299526213582306, 0.9870622384670245, 0.759944383215323, 1.0727648600406992, 1.0550252024609668, 0.976791569781049, 1.1077414427516377, 1.2882078253135203, 1.1107866505528392, 0.9672002611029203, 1.212004889597442, 1.0247017406283616, 1.4622157119276709, 0.8174608211126088, 0.9859834674580381, 0.8167587044675354, 0.988175832266449], \"Term\": [\"able\", \"accept\", \"acting\", \"admit\", \"adore\", \"afford\", \"anymore\", \"asks\", \"away\", \"babe\", \"baby\", \"badge\", \"bank\", \"battle\", \"black\", \"blazing\", \"blow\", \"break\", \"chase\", \"child\", \"cold\", \"come\", \"command\", \"compared\", \"confess\", \"creeping\", \"criticize\", \"danced\", \"dark\", \"dawn\", \"day\", \"dear\", \"deserted\", \"disease\", \"door\", \"doorway\", \"doubt\", \"drank\", \"dream\", \"easily\", \"eye\", \"father\", \"feel\", \"foggy\", \"friend\", \"gettin\", \"gladly\", \"glory\", \"gone\", \"good\", \"ground\", \"grown\", \"gun\", \"hall\", \"hand\", \"happens\", \"headed\", \"hear\", \"heart\", \"heaven\", \"heed\", \"home\", \"horse\", \"instrumental\", \"jack\", \"jane\", \"king\", \"knock\", \"knock\", \"know\", \"lane\", \"lend\", \"like\", \"lingered\", \"little\", \"long\", \"look\", \"lord\", \"loser\", \"love\", \"luck\", \"make\", \"mama\", \"mile\", \"mind\", \"mystery\", \"newborn\", \"night\", \"nighttime\", \"pretend\", \"rank\", \"reward\", \"riding\", \"right\", \"roam\", \"rock\", \"said\", \"santa\", \"seven\", \"shoot\", \"sink\", \"somebody\", \"son\", \"soon\", \"speak\", \"speaks\", \"special\", \"spin\", \"stone\", \"suit\", \"supposed\", \"sure\", \"tell\", \"tellin\", \"thing\", \"time\", \"tired\", \"tomorrow\", \"tonight\", \"trouble\", \"turnin\", \"unlock\", \"waking\", \"want\", \"watery\", \"went\", \"whispering\", \"window\", \"world\", \"yeah\", \"young\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 5, 6, 4, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el410214410719832972749133\", ldavis_el410214410719832972749133_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el410214410719832972749133\", ldavis_el410214410719832972749133_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el410214410719832972749133\", ldavis_el410214410719832972749133_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics           x          y\n",
       "topic                                                   \n",
       "0      86.212320        1       1  -55.687599  86.619881\n",
       "2       2.877156        1       2  -82.989761 -26.715849\n",
       "4       2.810667        1       3  105.066818 -12.061854\n",
       "5       2.799095        1       4   16.361940 -87.704376\n",
       "3       2.675050        1       5   60.537960  95.676468\n",
       "1       2.625713        1       6    8.657820  11.162947, topic_info=     Category      Freq          Term     Total  loglift  logprob\n",
       "term                                                             \n",
       "1085  Default  2.000000  instrumental  2.000000  30.0000  30.0000\n",
       "996   Default  4.000000        heaven  4.000000  29.0000  29.0000\n",
       "2052  Default  3.000000         stone  3.000000  28.0000  28.0000\n",
       "589   Default  6.000000          door  6.000000  27.0000  27.0000\n",
       "1156  Default  1.000000         knock  1.000000  26.0000  26.0000\n",
       "1794  Default  1.000000         santa  1.000000  25.0000  25.0000\n",
       "1040  Default  2.000000         horse  2.000000  24.0000  24.0000\n",
       "1101  Default  0.000000          jane  0.000000  23.0000  23.0000\n",
       "725   Default  3.000000        father  3.000000  22.0000  22.0000\n",
       "507   Default  4.000000          dark  4.000000  21.0000  21.0000\n",
       "214   Default  3.000000         break  3.000000  20.0000  20.0000\n",
       "382   Default  4.000000          cold  4.000000  19.0000  19.0000\n",
       "522   Default  4.000000          dear  4.000000  18.0000  18.0000\n",
       "151   Default  4.000000         black  4.000000  17.0000  17.0000\n",
       "1146  Default  4.000000          king  4.000000  16.0000  16.0000\n",
       "1299  Default  4.000000          mama  4.000000  15.0000  15.0000\n",
       "517   Default  3.000000           day  3.000000  14.0000  14.0000\n",
       "2401  Default  3.000000        window  3.000000  13.0000  13.0000\n",
       "2254  Default  5.000000       trouble  5.000000  12.0000  12.0000\n",
       "516   Default  2.000000          dawn  2.000000  11.0000  11.0000\n",
       "176   Default  3.000000          blow  3.000000  10.0000  10.0000\n",
       "335   Default  4.000000         child  4.000000   9.0000   9.0000\n",
       "926   Default  3.000000        ground  3.000000   8.0000   8.0000\n",
       "1973  Default  3.000000      somebody  3.000000   7.0000   7.0000\n",
       "1750  Default  3.000000          rock  3.000000   6.0000   6.0000\n",
       "2450  Default  3.000000          yeah  3.000000   5.0000   5.0000\n",
       "2460  Default  4.000000         young  4.000000   4.0000   4.0000\n",
       "1979  Default  4.000000          soon  4.000000   3.0000   3.0000\n",
       "2111  Default  3.000000          sure  3.000000   2.0000   2.0000\n",
       "1342  Default  3.000000          mile  3.000000   1.0000   1.0000\n",
       "...       ...       ...           ...       ...      ...      ...\n",
       "1942   Topic6  0.043501        sleigh  0.568518   1.0696  -7.5620\n",
       "1705   Topic6  0.041060      reindeer  0.605351   0.9490  -7.6198\n",
       "484    Topic6  0.034869         cupid  0.451693   1.0784  -7.7832\n",
       "331    Topic6  0.038344        cherry  0.678189   0.7670  -7.6882\n",
       "516    Topic6  0.040756          dawn  2.852711  -0.6086  -7.6272\n",
       "600    Topic6  0.035942         drank  0.915594   0.4022  -7.7529\n",
       "963    Topic6  0.034994       happens  0.694399   0.6519  -7.7796\n",
       "214    Topic6  0.038050         break  3.249222  -0.8075  -7.6959\n",
       "885    Topic6  0.036954         glory  1.940721  -0.3213  -7.7251\n",
       "1993   Topic6  0.035756       special  1.098089   0.2152  -7.7581\n",
       "499    Topic6  0.035625        danced  1.099741   0.2100  -7.7617\n",
       "299    Topic6  0.034689           cat  0.668830   0.6807  -7.7884\n",
       "17     Topic6  0.034723        afford  0.707989   0.6248  -7.7874\n",
       "982    Topic6  0.034634        headed  0.678411   0.6649  -7.7900\n",
       "1232   Topic6  0.034719      lingered  0.740674   0.5796  -7.7875\n",
       "500    Topic6  0.034103        dancer  0.523673   0.9083  -7.8054\n",
       "2254   Topic6  0.037226       trouble  5.118799  -1.2839  -7.7178\n",
       "11     Topic6  0.035079         adore  1.147095   0.1524  -7.7772\n",
       "784    Topic6  0.034543         foggy  0.785705   0.5154  -7.7926\n",
       "1423   Topic6  0.034698       newborn  0.943128   0.3373  -7.7881\n",
       "1723   Topic6  0.034517        reward  0.859124   0.4254  -7.7933\n",
       "397    Topic6  0.034486      compared  0.825885   0.4639  -7.7942\n",
       "1118   Topic6  0.034256        joyful  0.616277   0.7500  -7.8009\n",
       "2102   Topic6  0.034741          suit  1.291839   0.0239  -7.7869\n",
       "1177   Topic6  0.034586          lane  1.033138   0.2429  -7.7914\n",
       "1408   Topic6  0.034264        nation  0.649076   0.6984  -7.8007\n",
       "139    Topic6  0.034280     bethlehem  0.673796   0.6614  -7.8002\n",
       "1847   Topic6  0.034601         seven  2.121309  -0.4761  -7.7909\n",
       "2218   Topic6  0.034541       tonight  4.739223  -1.2817  -7.7926\n",
       "2215   Topic6  0.034541      tomorrow  2.796512  -0.7542  -7.7927\n",
       "\n",
       "[306 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "1         1  1.120746        able\n",
       "3         1  0.988594      accept\n",
       "8         1  1.330074      acting\n",
       "10        1  1.373713       admit\n",
       "11        1  0.871767       adore\n",
       "17        1  1.412451      afford\n",
       "42        1  1.003642     anymore\n",
       "64        1  1.374199        asks\n",
       "73        1  0.961350        away\n",
       "75        1  0.958486        babe\n",
       "76        1  0.960073        baby\n",
       "79        1  1.405476       badge\n",
       "90        1  0.859556        bank\n",
       "104       1  0.876033      battle\n",
       "151       1  0.968157       black\n",
       "157       1  1.212361     blazing\n",
       "176       1  0.820484        blow\n",
       "214       1  0.923298       break\n",
       "323       1  1.377976       chase\n",
       "335       1  0.955264       child\n",
       "382       1  0.906271        cold\n",
       "387       1  0.993038        come\n",
       "391       1  0.984791     command\n",
       "397       1  1.210823    compared\n",
       "406       1  1.337636     confess\n",
       "461       1  1.347269    creeping\n",
       "468       1  1.046859   criticize\n",
       "499       1  0.909305      danced\n",
       "507       1  0.986123        dark\n",
       "516       1  1.051631        dawn\n",
       "...     ...       ...         ...\n",
       "1973      1  1.037402    somebody\n",
       "1977      1  1.219084         son\n",
       "1979      1  0.865311        soon\n",
       "1991      1  0.966565       speak\n",
       "1992      1  0.905855      speaks\n",
       "1993      1  0.910673     special\n",
       "2000      1  0.793260        spin\n",
       "2052      1  0.890145       stone\n",
       "2102      1  0.774090        suit\n",
       "2110      1  0.742540    supposed\n",
       "2111      1  1.062182        sure\n",
       "2161      1  1.014658        tell\n",
       "2162      1  0.820107      tellin\n",
       "2180      1  1.029953       thing\n",
       "2205      1  0.987062        time\n",
       "2208      1  0.759944       tired\n",
       "2215      1  1.072765    tomorrow\n",
       "2218      1  1.055025     tonight\n",
       "2254      1  0.976792     trouble\n",
       "2271      1  1.107741      turnin\n",
       "2293      1  1.288208      unlock\n",
       "2328      1  1.110787      waking\n",
       "2337      1  0.967200        want\n",
       "2355      1  1.212005      watery\n",
       "2378      1  1.024702        went\n",
       "2388      1  1.462216  whispering\n",
       "2401      1  0.817461      window\n",
       "2428      1  0.985983       world\n",
       "2450      1  0.816759        yeah\n",
       "2460      1  0.988176       young\n",
       "\n",
       "[131 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 5, 6, 4, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, X, tfidf, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NMF vs. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF and LDA are both topic modeling techniques that both extract \"latent\" topics from a corpus. The main difference is unlike NMF, LDA is a probabilistic model with a Dirichlet prior. In other words, NMF’s topic-word probability distribution is fixed, while LDA’s distribution varies based on how the prior is tuned (i.e., hyperparameter $\\alpha$). While LDA seems to produce more coherent topics, it can have inferior performance on smaller datasets.\n",
    "\n",
    "Unlike NMF, reconstructing $X$ with LDA is not a closed-form solution. We need to use Monte Carlo simulations to sample from the distribution of $H$ (the distribution of topics for each sample), followed by the distribution of $W$ (the distribution of words for topic $k$).\n",
    "\n",
    "The following table is a sumamry of the key differences between NMF and LDA:\n",
    "\n",
    "|NMF|LDA|\n",
    "|---|----|\n",
    "|closed-form solution|not closed form|\n",
    "|deterministic|generative|\n",
    "|no prior|prior hyperparameters $\\alpha$ and $\\beta$|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
