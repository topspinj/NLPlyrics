{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Song Lyrics\n",
    "\n",
    "We will perform topic modeling using two techniques: Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) using tools from scikit-learn and gensim. All topic modeling code is contained in the `topic_modeling.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/lyrics_indie.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform TFIDF Vectorization\n",
    "\n",
    "Before we can start topic modelling, we must apply term frequency-inverse document frequency (TFIDF) vectorization to our tokenized dataset. TFIDF is used to determine how important a word is to a document in a collection or corpus ([ref](https://www.wikiwand.com/en/Tf%E2%80%93idf)). For example, let's say the word \"like\" is very popular across all songs. Using TFIDF, we downweight the importance of \"like\" because it is a word that occurs frequently within our corpus. Let's say \"democracy\" is another word within that song but it is very rare across all songs. Its importance would be upweighted using TFDIF because it doesn't occur very often in our corpus.\n",
    "\n",
    "Note: scikit-learn's `TfidfVectorizer` expects an array of strings. So, we will need to concatenate our tokenized words together as a string for TFIDF to work properly. That being said, our concatenated tokenized words are very different from our original lyrics because we filtered out stopwords and performed lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF matrix dimensions: (3148, 17681)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=3, max_df=0.9)\n",
    "X = tfidf.fit_transform(data['processed_lyrics'])\n",
    "print(\"TFIDF matrix dimensions:\",X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scikit-learn's '`TfidfVectorizer`, you can specify a minimum and maximum document frequency (`min_df`, `max_df`). I set `min_df` to be 3, which means that a word must be mentioned in at least 3 documents in order for the vectorizer to include it. I set `max_df` to be 0.9 which will ignore words that appear in more than 90% of documents. You can think of it as a filter for corpus-specific stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3148x3570 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 119558 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our TFIDF matrix, we can start topic modeling with NMF and LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "NMF was first published in the context of machine learning of facial images by Lee and Seung in 1999. It starts with a document-word matrix, $X_{ij}$, which represents the number of occurences of word $w_i$ in document $d_j$. We create our document-word matrix $X$ using tf-idf or count vectorization. This matrix gets factorized into two smaller matrices: a word-topic matrix $W_{ik}$ and topic-document matrix $H_{kj}$. $W_{ik}$ represents the $k$ topics discovered from the documents, while $H_{kj}$ represents the coefficient weights for the topics in each document. By reducing the dimensionality of our original document-word matrix, we are able to extract information about $k$ topics. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/matrix_factorization.png\" width=\"50%\"/>\n",
    "\n",
    "The process of factorizing $W$ and $H$ involves optimizing over an objective function, which in this case is the reconstruction error between $X$ and the product of its factors $W$ and $H$. $W$ and $H$ are updated iteratively until convergence (i.e., reconstruction error can no longer be minimized). In our example, a song represents one \"document\" in our $X$ matrix. Our goal is to reduce the dimensionality of our song-word matrix, $X$, so that we can extract meaningful $k$ topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=8, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "k_topics = 8\n",
    "nmf = NMF(n_components=k_topics)\n",
    "nmf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>time</td>\n",
       "      <td>instrumental</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>want</td>\n",
       "      <td>let</td>\n",
       "      <td>gon</td>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>purely</td>\n",
       "      <td>heart</td>\n",
       "      <td>heart</td>\n",
       "      <td>say</td>\n",
       "      <td>come</td>\n",
       "      <td>wan</td>\n",
       "      <td>pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>way</td>\n",
       "      <td>team</td>\n",
       "      <td>need</td>\n",
       "      <td>tell</td>\n",
       "      <td>need</td>\n",
       "      <td>home</td>\n",
       "      <td>tonight</td>\n",
       "      <td>le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>lyric</td>\n",
       "      <td>said</td>\n",
       "      <td>feel</td>\n",
       "      <td>tell</td>\n",
       "      <td>leave</td>\n",
       "      <td>make</td>\n",
       "      <td>por</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got</td>\n",
       "      <td>song</td>\n",
       "      <td>like</td>\n",
       "      <td>night</td>\n",
       "      <td>feel</td>\n",
       "      <td>shine</td>\n",
       "      <td>got</td>\n",
       "      <td>qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>say</td>\n",
       "      <td>devil</td>\n",
       "      <td>anymore</td>\n",
       "      <td>said</td>\n",
       "      <td>girl</td>\n",
       "      <td>light</td>\n",
       "      <td>run</td>\n",
       "      <td>los</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>away</td>\n",
       "      <td>ooh</td>\n",
       "      <td>darling</td>\n",
       "      <td>make</td>\n",
       "      <td>really</td>\n",
       "      <td>long</td>\n",
       "      <td>baby</td>\n",
       "      <td>comme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thing</td>\n",
       "      <td>motion</td>\n",
       "      <td>baby</td>\n",
       "      <td>alright</td>\n",
       "      <td>think</td>\n",
       "      <td>rain</td>\n",
       "      <td>try</td>\n",
       "      <td>tout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eye</td>\n",
       "      <td>frozen</td>\n",
       "      <td>life</td>\n",
       "      <td>think</td>\n",
       "      <td>hear</td>\n",
       "      <td>heart</td>\n",
       "      <td>lose</td>\n",
       "      <td>pero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>life</td>\n",
       "      <td>captivating</td>\n",
       "      <td>hold</td>\n",
       "      <td>baby</td>\n",
       "      <td>ooh</td>\n",
       "      <td>sun</td>\n",
       "      <td>stop</td>\n",
       "      <td>pour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1             2        3        4       5      6        7      8\n",
       "0   time  instrumental     love     know    want    let      gon    que\n",
       "1   like        purely    heart    heart     say   come      wan     pa\n",
       "2    way          team     need     tell    need   home  tonight     le\n",
       "3    day         lyric     said     feel    tell  leave     make    por\n",
       "4    got          song     like    night    feel  shine      got    qui\n",
       "5    say         devil  anymore     said    girl  light      run    los\n",
       "6   away           ooh  darling     make  really   long     baby  comme\n",
       "7  thing        motion     baby  alright   think   rain      try   tout\n",
       "8    eye        frozen     life    think    hear  heart     lose   pero\n",
       "9   life   captivating     hold     baby     ooh    sun     stop   pour"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 10\n",
    "word_dict = dict()\n",
    "\n",
    "for i in range(0,k_topics):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':nmf.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    word_dict[i+1] = list(sorted_topic['word'])\n",
    "\n",
    "pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We looked at the top 10 most \"relevant\" words across 8 topics in our indie lyric corpus. Some of the topics are hard to summarize, but others are quite obvious. For example, Topic 3 is clearly about `love` and Topic 8 captures lyrics from non-English songs. \n",
    "\n",
    "Note that results can change if you try out different $k$ topics. Choosing a small $k$ can result in extremely broad topics, while choosing a large $k$ can end up in over-clustering, which produces many highly-similar topics ([ref](https://arxiv.org/pdf/1404.4606.pdf)). There are strategies to identify optimal $k$ (e.g., term-centric stability analysis, k-clustering, etc.), but this is outside the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=15, mean_change_tol=0.001,\n",
       "             n_components=6, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "k_topics = 6\n",
    "lda = LDA(n_components=k_topics, max_iter=15, learning_method='online')\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: que pa le non qui por amor los tout nous dans sem pour che comme quand sol suis plus est\n",
      "Topic 1: know love like time come say let got want way day feel make away heart thing life night need gon\n",
      "Topic 2: handa kita bein marilyn henry nãº monroe ano kong lagi held marry ohh chorus man paris smoked eaten dinner married\n",
      "Topic 3: kau aku yang tak pergi hanya ore dan revoir kita kini lagi total lain mental outta repeat french door fake\n",
      "Topic 4: mmmm dash rejoice mmmmm grandmother believer translation dialect dawning original pit mmm english wade person dawn hill silence shadow living\n",
      "Topic 5: uptown congregation rotation station wilt working fork afloat dreaming beak childhood calming maid leaving spoken hospital wise course mobile giro\n"
     ]
    }
   ],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "top_n_words = 20\n",
    "for i in range(0,k_topics):\n",
    "    topic = pd.DataFrame(data={'word':tfidf_features, 'weight':lda.components_[i]})\n",
    "    sorted_topic = topic.sort_values('weight', ascending=False).head(top_n_words)\n",
    "    print(\"Topic %s:\" % i, ' '.join(sorted_topic['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el9863445686971363202782801\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el9863445686971363202782801_data = {\"mdsDat\": {\"Freq\": [86.75256450087788, 3.535217245669855, 3.393968023227513, 3.194113604401311, 3.124136625823436], \"cluster\": [1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5], \"x\": [146.7118682861328, -238.9890594482422, 351.09271240234375, 42.240325927734375, -183.07199096679688], \"y\": [-162.30723571777344, 93.47671508789062, 113.34552764892578, 288.3572998046875, -257.0924987792969]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [4.0, 3.0, 6.0, 1.0, 5.0, 1.0, 3.0, 1.0, 3.0, 3.0, 2.0, 8.0, 1.0, 1.0, 2.0, 2.0, 1.0, 0.0, 1.0, 0.0, 5.0, 3.0, 2.0, 3.0, 3.0, 4.0, 0.0, 1.0, 3.0, 3.0, 16.812592600055137, 17.052626719604568, 16.278436343751892, 13.482252149661662, 15.9445687451332, 12.947240031040721, 13.014556614163554, 15.014386754697625, 13.215019363221138, 12.8434719319281, 11.007419865108465, 11.037113389112452, 10.35113263084098, 10.21678444592976, 9.930365226868936, 9.103312178249716, 8.766609227168743, 8.255965048612227, 10.130359977585318, 8.17726742617644, 7.480337020797317, 8.6115368101658, 7.287286525510316, 7.5668593945446245, 7.082726994822448, 6.971753545799436, 6.873315995430145, 7.163720093060923, 7.061694885822879, 6.701183458895206, 8.748601730505664, 7.153681178438952, 0.18541956799936604, 0.18531299735210882, 0.1170002802054811, 0.1169293631020556, 0.11688204953404321, 0.11683522827362001, 0.11682923865183742, 0.11681710882179494, 0.11689024228063773, 0.11682025946542303, 0.1137561301189305, 0.11479903987410578, 0.1141208705646106, 0.11342083527653209, 0.112778856078926, 0.083363151189513, 0.08325834049353638, 0.08324185015307635, 0.08325581334196176, 0.0832819829341109, 0.08322102268051422, 0.08319588284879376, 0.08317184107879003, 0.11285477298245762, 0.08314459962587556, 0.08320491265364026, 0.08306578628330005, 0.08307707501488132, 0.08298401690341722, 0.08300315866086223, 0.11027526694885051, 0.1110303047792716, 0.10732909284129954, 0.10716769483917293, 0.10631894001405362, 0.14511307451162975, 0.10604718360025983, 0.10331727247497932, 0.10261124564786722, 0.1068165001944592, 0.10020165991704794, 0.09997999490926142, 0.10013415300879933, 0.09736789545363884, 0.21963280803440752, 0.3798823099563647, 0.38186798676407646, 0.3084293258064049, 0.19651488475694376, 0.5893004955595881, 0.12453015877804435, 0.15729943674033856, 0.15658947282104937, 0.12016720395574461, 0.21541037199593416, 0.14638315651179995, 0.12577979592886326, 0.07829601218065034, 0.2651707117435431, 0.09477540675376803, 0.09003991300325848, 0.10123051005329568, 0.05423058379516934, 0.054201026403199674, 0.05417044140823027, 0.05415656807580459, 0.09428392109204135, 0.1297608810483739, 0.053756461615926315, 0.089553149229501, 0.10291498347553167, 0.07372790907902024, 0.040721196502220953, 0.04069181709931, 0.08134395743868114, 0.33546313545466117, 0.1494924531098659, 0.3073054763322894, 0.1436477939339836, 0.2186324924792956, 0.08011937344446071, 0.13204335234817213, 0.12425114945172923, 0.1183291335779436, 0.10711821769612204, 0.11589106007604917, 0.09317964757776628, 0.5754790730782468, 0.17136331277624842, 0.24278533950867803, 0.1797222795636617, 0.11218028138899193, 0.11222566644224465, 0.11220631775331476, 0.11226310074840547, 0.10458736908879397, 0.10445491672969363, 0.10443778650234926, 0.10932558670262084, 0.10933937015793595, 0.09821190930495924, 0.15575470507212769, 0.0943362366153184, 0.09019619800697597, 0.26554409746305896, 0.1000673471988025, 0.08601117461390331, 0.09100409469925624, 0.07938627146493525, 0.07423654328087913, 0.040105277777569844, 0.04009729145239275, 0.04006114118233834, 0.04004946548815495, 0.04010534733968627, 0.04014258243901593, 0.04011838259068562, 0.0817643213421553, 0.08583290449650957, 0.11856576135975316, 0.06597732554522266, 0.0823263702347328, 0.06433302018017018, 0.07979172054844301, 0.12788954443193382, 0.07293139159427514, 0.32299405813396975, 0.2560403570728457, 0.11915837131871782, 0.1191142177831106, 0.11914934623126308, 0.11910034687697241, 0.11626104229340314, 0.11337252240037013, 0.11254227586917287, 0.10895361583143685, 0.10222467198558303, 0.07279297822536779, 0.08602438330438031, 0.2237064054999492, 0.039785712993715645, 0.0397478810501537, 0.03974405005158939, 0.03972917623983122, 0.03979460750944643, 0.03980950777373621, 0.03979685279716409, 0.0397943663810484, 0.03975169412071525, 0.039752204201015745, 0.03975258688183244, 0.03972862292580639, 0.0398031061580709, 0.03968952929531452, 0.039769391517863475, 0.03981880777206857, 0.03979477013542151, 0.06482170257582924, 0.11742352065503356, 0.06369441660943004, 0.06054304367470937, 0.039834593444719096, 0.09092656943943907, 0.043728536236715895, 0.049539570380210576, 0.05456054638731019, 0.05365032317860193, 0.05307719161569893, 0.050993130122372506, 0.04977029224226356, 0.04800694319300186, 0.04730165325473933, 0.04730841588347521, 0.04617590279080379, 0.04563764259646662], \"Term\": [\"heaven\", \"stone\", \"door\", \"knock\", \"everybody\", \"wan\", \"thousand\", \"mighty\", \"hang\", \"hair\", \"dawn\", \"gone\", \"hurry\", \"shore\", \"low\", \"break\", \"weekend\", \"quinn\", \"block\", \"knockin\", \"hold\", \"pretty\", \"ready\", \"dark\", \"ship\", \"dear\", \"brand\", \"seven\", \"gold\", \"mother\", \"got\", \"know\", \"love\", \"baby\", \"like\", \"said\", \"gon\", \"come\", \"time\", \"say\", \"want\", \"man\", \"night\", \"day\", \"tell\", \"heart\", \"way\", \"right\", \"let\", \"make\", \"instrumental\", \"away\", \"thing\", \"long\", \"eye\", \"old\", \"good\", \"mind\", \"hand\", \"friend\", \"gone\", \"lord\", \"success\", \"failure\", \"matchstick\", \"banker\", \"ideal\", \"dangles\", \"quotation\", \"horseman\", \"raven\", \"madam\", \"ceremony\", \"conclusion\", \"statue\", \"perfection\", \"valentine\", \"limited\", \"idolize\", \"disillusioned\", \"honesty\", \"despise\", \"insure\", \"obscenity\", \"reappear\", \"crumble\", \"invest\", \"startle\", \"bitterly\", \"propaganda\", \"stuffed\", \"plier\", \"cloak\", \"rainy\", \"niece\", \"dagger\", \"grudge\", \"speaks\", \"argue\", \"wink\", \"expecting\", \"ramble\", \"violence\", \"candle\", \"tremble\", \"hammer\", \"stoned\", \"knockin\", \"weekend\", \"brand\", \"dandy\", \"knock\", \"cheatin\", \"plow\", \"drank\", \"apron\", \"driftin\", \"unkind\", \"balance\", \"breakfast\", \"hurry\", \"afford\", \"headed\", \"foggy\", \"subjugation\", \"mortician\", \"libido\", \"tattered\", \"lingered\", \"danced\", \"sniffin\", \"compared\", \"driver\", \"whip\", \"reappear\", \"disillusioned\", \"reward\", \"stone\", \"block\", \"heaven\", \"shore\", \"door\", \"dew\", \"thousand\", \"hang\", \"hair\", \"low\", \"gone\", \"maggie\", \"quinn\", \"evidently\", \"eskimo\", \"pigeon\", \"doze\", \"jotting\", \"haste\", \"monument\", \"whirling\", \"dreamin\", \"myth\", \"feeding\", \"recite\", \"swirling\", \"building\", \"swayed\", \"facing\", \"mighty\", \"despair\", \"skirt\", \"dude\", \"whispering\", \"asks\", \"subjugation\", \"libido\", \"mortician\", \"tattered\", \"breakfast\", \"obscenity\", \"disillusioned\", \"incomprehensible\", \"limb\", \"act\", \"chase\", \"meat\", \"gladly\", \"note\", \"everybody\", \"joy\", \"mood\", \"ramble\", \"congressman\", \"prophesize\", \"rapidly\", \"critic\", \"stalled\", \"writer\", \"rattle\", \"drenched\", \"senator\", \"ragin\", \"admit\", \"wan\", \"tattered\", \"mortician\", \"subjugation\", \"libido\", \"breakfast\", \"idolize\", \"honesty\", \"despise\", \"limited\", \"obscenity\", \"insure\", \"reappear\", \"startle\", \"disillusioned\", \"plier\", \"guillotine\", \"propaganda\", \"pen\", \"dawn\", \"accept\", \"criticize\", \"baptized\", \"break\", \"stall\", \"happens\", \"command\", \"loser\", \"heed\", \"lend\", \"battle\", \"sink\", \"roam\", \"grown\", \"hall\", \"doorway\"], \"Total\": [4.0, 3.0, 6.0, 1.0, 5.0, 1.0, 3.0, 1.0, 3.0, 3.0, 2.0, 8.0, 1.0, 1.0, 2.0, 2.0, 1.0, 0.0, 1.0, 0.0, 5.0, 3.0, 2.0, 3.0, 3.0, 4.0, 0.0, 1.0, 3.0, 3.0, 16.98857086271579, 17.233993606840883, 16.46469383074004, 13.65583564082498, 16.15233732215361, 13.120320051121762, 13.189659455331997, 15.226234930074817, 13.40350944299602, 13.031301643851489, 11.180640837122748, 11.21094998151542, 10.526129565598614, 10.393043047676917, 10.110918700516953, 9.282876765523477, 8.940740930052254, 8.426510438018836, 10.340132480715333, 8.346734589452518, 7.645148908881279, 8.805681357938425, 7.457301902954379, 7.744779169460744, 7.2526190221776785, 7.14160190662908, 7.042464580364684, 7.344249067632969, 7.240314932067186, 6.870847630122472, 8.988703572509175, 7.341305743520998, 0.5364006156819103, 0.5361652091438471, 0.39419244125754255, 0.3941069663647125, 0.39408484852646386, 0.39402700331536733, 0.3940368819573543, 0.39403241757810226, 0.39435225956538816, 0.39453125998283856, 0.4373692753934444, 0.4535481287991779, 0.4558686985971524, 0.4600641850939653, 0.48667108072329435, 0.3676529503973093, 0.36772535175254967, 0.36765696547896953, 0.3677913580404675, 0.36795400842250525, 0.36780583965053193, 0.3677917485838367, 0.36786695407766073, 0.4996770051094265, 0.3681756892889101, 0.3685946788346117, 0.3685897239369559, 0.3687555504549776, 0.36834565981975875, 0.3684444617343037, 0.4917305412616536, 0.5337453866461029, 0.5491261552862214, 0.5820799865563965, 0.5897573216501699, 1.0863545604380798, 0.5982449025326995, 0.576072784953535, 0.5866718198714406, 0.7420765438698282, 0.6401852405766498, 0.6437154025815519, 0.6876980073709841, 0.6404491712482061, 0.5014688514971024, 0.9054399917733208, 1.0299900916228326, 0.9297502516817083, 0.5960844803474422, 1.8470595756653456, 0.41391910636413676, 0.5370787666441639, 0.5743896607982952, 0.4696498092140831, 0.8704487658735308, 0.6636026617307798, 0.5815730597474253, 0.362420332336135, 1.324964817247845, 0.5609952666704034, 0.544223210415256, 0.6125006699567679, 0.3338386713803096, 0.3338379781528062, 0.3338152991191726, 0.33380992887625194, 0.6011760513213824, 0.8404049781101619, 0.3715686234952237, 0.6363291120244841, 0.7346576093718399, 0.5476597191138315, 0.36786695407766073, 0.36765696547896953, 0.7412965091280881, 3.2315346086966796, 1.5454673059618207, 4.332044057817987, 1.9574680894781804, 6.506178118602025, 0.7823945314048272, 3.215251448748445, 3.0978417184471483, 3.1122810024162932, 2.7100910521820296, 8.988703572509175, 1.5730722220156608, 0.8560085261268371, 0.4564330711309314, 0.6931802320836533, 0.5709088693211842, 0.39282451400191587, 0.3931204383418524, 0.3931199216318587, 0.39333422324700607, 0.3916394437617985, 0.39161872886401466, 0.39199263041862586, 0.44170354565925385, 0.45218047116560184, 0.4445438075208726, 0.7598924346997249, 0.46773014973286064, 0.48511408998707406, 1.511983708383256, 0.6059489077001252, 0.5413023807696354, 0.643702760952892, 0.576459808205184, 0.6165273697466664, 0.3338386713803096, 0.3338152991191726, 0.3338379781528062, 0.33380992887625194, 0.362420332336135, 0.3677917485838367, 0.36765696547896953, 0.7882000073465918, 0.854760826743715, 1.254701182334509, 0.6535872022539762, 0.8967378370092515, 0.6680781315902308, 0.9607987988269812, 5.113555494930193, 1.0857626887910454, 0.6420987261372855, 0.7420765438698282, 0.40446796617158004, 0.4044666650667954, 0.40511689699345166, 0.4050155240729585, 0.43829053230593085, 0.4512660179553474, 0.47977834440542955, 0.49240018090179544, 0.5316490977325167, 0.5293763034940316, 0.6381001142702903, 1.8575046862756115, 0.33380992887625194, 0.3338379781528062, 0.3338386713803096, 0.3338152991191726, 0.362420332336135, 0.36772535175254967, 0.3677913580404675, 0.36795400842250525, 0.3676529503973093, 0.3677917485838367, 0.36780583965053193, 0.36786695407766073, 0.3685946788346117, 0.36765696547896953, 0.3684444617343037, 0.36895461644869926, 0.3687555504549776, 0.7692395320649147, 2.4328046389597526, 0.8654841710553145, 0.8925144209913469, 0.3695662508555846, 2.8646763111844673, 0.4695986161950908, 0.6609866892084392, 0.9469886151584725, 0.9006427683520055, 1.033082145121697, 0.9743807917828652, 1.0607891662707754, 1.165446573022474, 1.1151003754748758, 1.1612070112350579, 1.121336742246025, 1.2465167447687777], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.1317, 0.1315, 0.1307, 0.1293, 0.1292, 0.1288, 0.1287, 0.1281, 0.1279, 0.1276, 0.1265, 0.1265, 0.1253, 0.125, 0.1241, 0.1226, 0.1224, 0.1217, 0.1216, 0.1216, 0.1203, 0.1198, 0.119, 0.1189, 0.1184, 0.118, 0.1178, 0.1172, 0.1171, 0.1171, 0.115, 0.1162, 2.2801, 2.28, 2.1277, 2.1273, 2.127, 2.1267, 2.1267, 2.1266, 2.1264, 2.1253, 1.9957, 1.9685, 1.9574, 1.9421, 1.8802, 1.8585, 1.857, 1.857, 1.8568, 1.8567, 1.8563, 1.8561, 1.8556, 1.8545, 1.8544, 1.854, 1.8523, 1.852, 1.852, 1.852, 1.8474, 1.7723, 1.71, 1.6502, 1.6291, 1.3293, 1.6123, 1.624, 1.5989, 1.4041, 1.4878, 1.4801, 1.4156, 1.4587, 2.5576, 2.5146, 2.3909, 2.2797, 2.2735, 2.2408, 2.182, 2.1552, 2.0835, 2.0201, 1.9867, 1.8717, 1.852, 1.8509, 1.7744, 1.605, 1.5841, 1.583, 1.5658, 1.5652, 1.5647, 1.5645, 1.5306, 1.515, 1.4499, 1.4223, 1.4177, 1.3779, 1.1822, 1.182, 1.1735, 1.118, 1.0473, 0.7372, 0.7711, -0.0099, 1.1043, 0.1906, 0.167, 0.1135, 0.1524, -0.9679, 0.5569, 3.0468, 2.4642, 2.3947, 2.288, 2.1906, 2.1903, 2.1901, 2.19, 2.1235, 2.1223, 2.1212, 2.0476, 2.0242, 1.9339, 1.859, 1.8428, 1.7615, 1.7045, 1.6429, 1.6044, 1.4875, 1.4613, 1.327, 1.3247, 1.3246, 1.3236, 1.3234, 1.2426, 1.2288, 1.2285, 1.1779, 1.1454, 1.0847, 1.1507, 1.0558, 1.1035, 0.9555, -0.2446, 0.7433, 2.7789, 2.4019, 2.2439, 2.2435, 2.2422, 2.2421, 2.139, 2.0846, 2.016, 1.9576, 1.8172, 1.4819, 1.4621, 1.3494, 1.3389, 1.3379, 1.3378, 1.3375, 1.2569, 1.2428, 1.2423, 1.2418, 1.2415, 1.2412, 1.2411, 1.2404, 1.2403, 1.2399, 1.2398, 1.2397, 1.2396, 0.9923, 0.435, 0.8568, 0.7753, 1.2384, 0.0159, 1.0921, 0.8751, 0.612, 0.6454, 0.4975, 0.5159, 0.4067, 0.2765, 0.3059, 0.2655, 0.2762, 0.1586], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.1669, -5.1528, -5.1992, -5.3877, -5.2199, -5.4282, -5.423, -5.2801, -5.4077, -5.4362, -5.5905, -5.5878, -5.652, -5.665, -5.6935, -5.7804, -5.8181, -5.8781, -5.6735, -5.8877, -5.9768, -5.836, -6.0029, -5.9653, -6.0314, -6.0472, -6.0614, -6.02, -6.0344, -6.0868, -5.8202, -6.0214, -6.4739, -6.4745, -6.9344, -6.935, -6.9354, -6.9358, -6.9358, -6.9359, -6.9353, -6.9359, -6.9625, -6.9533, -6.9593, -6.9654, -6.9711, -7.2733, -7.2746, -7.2748, -7.2746, -7.2743, -7.275, -7.2753, -7.2756, -6.9704, -7.276, -7.2752, -7.2769, -7.2768, -7.2779, -7.2777, -6.9936, -6.9867, -7.0206, -7.0221, -7.0301, -6.719, -7.0326, -7.0587, -7.0656, -7.0254, -7.0893, -7.0916, -7.09, -7.118, -6.2638, -5.7159, -5.7107, -5.9243, -6.375, -5.2768, -6.8312, -6.5976, -6.6021, -6.8669, -6.2832, -6.6695, -6.8212, -7.2953, -6.0754, -7.1042, -7.1555, -7.0384, -7.6625, -7.6631, -7.6636, -7.6639, -7.1094, -6.7901, -7.6713, -7.1609, -7.0219, -7.3554, -7.949, -7.9497, -7.2571, -5.8402, -6.6485, -5.9279, -6.6884, -6.2684, -7.2722, -6.7726, -6.8335, -6.8823, -6.9818, -6.9031, -7.1212, -5.2399, -6.4513, -6.1029, -6.4037, -6.875, -6.8746, -6.8747, -6.8742, -6.945, -6.9463, -6.9465, -6.9007, -6.9006, -7.0079, -6.5468, -7.0482, -7.0931, -6.0133, -6.9892, -7.1406, -7.0842, -7.2207, -7.2878, -7.9036, -7.9038, -7.9047, -7.905, -7.9036, -7.9026, -7.9032, -7.1912, -7.1427, -6.8196, -7.4058, -7.1844, -7.431, -7.2156, -6.7439, -7.3055, -5.7953, -6.0276, -6.7925, -6.7928, -6.7925, -6.7929, -6.8171, -6.8422, -6.8496, -6.882, -6.9457, -7.2853, -7.1183, -6.1626, -7.8894, -7.8904, -7.8905, -7.8908, -7.8892, -7.8888, -7.8891, -7.8892, -7.8903, -7.8903, -7.8902, -7.8908, -7.889, -7.8918, -7.8898, -7.8886, -7.8892, -7.4013, -6.8071, -7.4188, -7.4696, -7.8882, -7.0629, -7.7949, -7.6701, -7.5736, -7.5904, -7.6012, -7.6412, -7.6655, -7.7016, -7.7164, -7.7162, -7.7405, -7.7522]}, \"token.table\": {\"Topic\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [1.15542263329977, 0.7970025166784258, 1.0220674169508068, 0.9519739649718456, 0.9426943937554709, 0.6470534809389905, 1.0472387362883522, 0.985141768065856, 1.0559789040680878, 1.1204300753922443, 1.1899025184842706, 1.0503777517080382, 0.8220964264747471, 0.9621821014428713, 0.9359195692340646, 1.278127542896359, 0.9222003902483402, 0.8022355128374106, 1.1488326932102193, 1.3611783057076587, 0.9777932409176401, 0.9651685795979082, 1.0187971523791783, 0.9144536705887084, 0.9856205949839496, 1.001256736013119, 0.9939702103034951, 1.0006727544875063, 0.8611728919345756, 0.9639232439715048, 0.8917927704723303, 0.966808773606955, 0.9684161660473107, 0.9695270364275351, 0.9233516433844317, 0.9679772365848024, 0.9616868029773276, 0.7547370216796799, 1.268713512660847, 0.9156132971940131, 0.9210115712425716, 0.5414010534228606, 0.5414010534228606, 0.9864225546220464, 1.026292809169871, 0.967105597404125, 0.9905687134242378, 1.1699179100305592, 1.0329539196605688, 0.9535088504082242, 1.1103181362681656, 0.9717763454627718, 0.7379825848986513, 0.6356987212695467, 0.9584586540117527, 0.9811835766047272, 1.1151531236098418, 0.6613827876950386, 0.9531267166373594, 0.8580041323671214, 0.9500168070020623, 1.0408006350766454, 0.98017224868028, 1.299985190978994, 1.0588783082074236, 1.1682126631666603, 0.8282222424945023, 1.3489878715012413, 0.9493846900023406, 0.8967802558349428, 0.9908294881029617, 0.9975979649073462, 1.0000845595828092, 0.9114547633889951, 1.0217280224134626, 0.8580401909000392, 0.9205097823649245, 0.928351499602209, 0.9890298098716508, 0.9386772979147849, 0.9330529968869984, 0.9698952393988969, 1.0767132997172129, 0.983843427245872, 1.0066279819996304, 0.9708831260933969], \"Term\": [\"accept\", \"act\", \"away\", \"baby\", \"battle\", \"block\", \"break\", \"come\", \"command\", \"criticize\", \"danced\", \"dark\", \"dawn\", \"day\", \"dear\", \"dew\", \"door\", \"doorway\", \"driftin\", \"driver\", \"everybody\", \"eye\", \"friend\", \"gold\", \"gon\", \"gone\", \"good\", \"got\", \"grown\", \"hair\", \"hall\", \"hand\", \"hang\", \"heart\", \"heaven\", \"heed\", \"hold\", \"hurry\", \"incomprehensible\", \"instrumental\", \"joy\", \"knock\", \"knock\", \"know\", \"lend\", \"let\", \"like\", \"limb\", \"long\", \"lord\", \"loser\", \"love\", \"low\", \"maggie\", \"make\", \"man\", \"meat\", \"mighty\", \"mind\", \"mother\", \"night\", \"note\", \"old\", \"pen\", \"pretty\", \"quinn\", \"ready\", \"reward\", \"right\", \"roam\", \"said\", \"say\", \"seven\", \"ship\", \"shore\", \"sink\", \"speaks\", \"stone\", \"tell\", \"thing\", \"thousand\", \"time\", \"wan\", \"want\", \"way\", \"weekend\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 5, 3, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el9863445686971363202782801\", ldavis_el9863445686971363202782801_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el9863445686971363202782801\", ldavis_el9863445686971363202782801_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el9863445686971363202782801\", ldavis_el9863445686971363202782801_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics           x           y\n",
       "topic                                                    \n",
       "1      86.752565        1       1  146.711868 -162.307236\n",
       "0       3.535217        1       2 -238.989059   93.476715\n",
       "4       3.393968        1       3  351.092712  113.345528\n",
       "2       3.194114        1       4   42.240326  288.357300\n",
       "3       3.124137        1       5 -183.071991 -257.092499, topic_info=     Category      Freq           Term     Total  loglift  logprob\n",
       "term                                                              \n",
       "1054  Default  4.000000         heaven  4.000000  30.0000  30.0000\n",
       "2187  Default  3.000000          stone  3.000000  29.0000  29.0000\n",
       "621   Default  6.000000           door  6.000000  28.0000  28.0000\n",
       "1234  Default  1.000000          knock  1.000000  27.0000  27.0000\n",
       "719   Default  5.000000      everybody  5.000000  26.0000  26.0000\n",
       "2481  Default  1.000000            wan  1.000000  25.0000  25.0000\n",
       "2328  Default  3.000000       thousand  3.000000  24.0000  24.0000\n",
       "1433  Default  1.000000         mighty  1.000000  23.0000  23.0000\n",
       "1014  Default  3.000000           hang  3.000000  22.0000  22.0000\n",
       "1002  Default  3.000000           hair  3.000000  21.0000  21.0000\n",
       "544   Default  2.000000           dawn  2.000000  20.0000  20.0000\n",
       "948   Default  8.000000           gone  8.000000  19.0000  19.0000\n",
       "1128  Default  1.000000          hurry  1.000000  18.0000  18.0000\n",
       "2019  Default  1.000000          shore  1.000000  17.0000  17.0000\n",
       "1363  Default  2.000000            low  2.000000  16.0000  16.0000\n",
       "231   Default  2.000000          break  2.000000  15.0000  15.0000\n",
       "2524  Default  1.000000        weekend  1.000000  14.0000  14.0000\n",
       "1761  Default  0.000000          quinn  0.000000  13.0000  13.0000\n",
       "184   Default  1.000000          block  1.000000  12.0000  12.0000\n",
       "1236  Default  0.000000        knockin  0.000000  11.0000  11.0000\n",
       "1083  Default  5.000000           hold  5.000000  10.0000  10.0000\n",
       "1712  Default  3.000000         pretty  3.000000   9.0000   9.0000\n",
       "1799  Default  2.000000          ready  2.000000   8.0000   8.0000\n",
       "535   Default  3.000000           dark  3.000000   7.0000   7.0000\n",
       "2011  Default  3.000000           ship  3.000000   6.0000   6.0000\n",
       "550   Default  4.000000           dear  4.000000   5.0000   5.0000\n",
       "227   Default  0.000000          brand  0.000000   4.0000   4.0000\n",
       "1976  Default  1.000000          seven  1.000000   3.0000   3.0000\n",
       "944   Default  3.000000           gold  3.000000   2.0000   2.0000\n",
       "1472  Default  3.000000         mother  3.000000   1.0000   1.0000\n",
       "...       ...       ...            ...       ...      ...      ...\n",
       "1137   Topic5  0.039810        idolize  0.367725   1.2428  -7.8888\n",
       "1093   Topic5  0.039797        honesty  0.367791   1.2423  -7.8891\n",
       "580    Topic5  0.039794        despise  0.367954   1.2418  -7.8892\n",
       "1311   Topic5  0.039752        limited  0.367653   1.2415  -7.8903\n",
       "1545   Topic5  0.039752      obscenity  0.367792   1.2412  -7.8903\n",
       "1156   Topic5  0.039753         insure  0.367806   1.2411  -7.8902\n",
       "1804   Topic5  0.039729       reappear  0.367867   1.2404  -7.8908\n",
       "2167   Topic5  0.039803        startle  0.368595   1.2403  -7.8890\n",
       "607    Topic5  0.039690  disillusioned  0.367657   1.2399  -7.8918\n",
       "1672   Topic5  0.039769          plier  0.368444   1.2398  -7.8898\n",
       "989    Topic5  0.039819     guillotine  0.368955   1.2397  -7.8886\n",
       "1728   Topic5  0.039795     propaganda  0.368756   1.2396  -7.8892\n",
       "1624   Topic5  0.064822            pen  0.769240   0.9923  -7.4013\n",
       "544    Topic5  0.117424           dawn  2.432805   0.4350  -6.8071\n",
       "3      Topic5  0.063694         accept  0.865484   0.8568  -7.4188\n",
       "493    Topic5  0.060543      criticize  0.892514   0.7753  -7.4696\n",
       "99     Topic5  0.039835       baptized  0.369566   1.2384  -7.8882\n",
       "231    Topic5  0.090927          break  2.864676   0.0159  -7.0629\n",
       "2152   Topic5  0.043729          stall  0.469599   1.0921  -7.7949\n",
       "1020   Topic5  0.049540        happens  0.660987   0.8751  -7.6701\n",
       "415    Topic5  0.054561        command  0.946989   0.6120  -7.5736\n",
       "1351   Topic5  0.053650          loser  0.900643   0.6454  -7.5904\n",
       "1056   Topic5  0.053077           heed  1.033082   0.4975  -7.6012\n",
       "1291   Topic5  0.050993           lend  0.974381   0.5159  -7.6412\n",
       "112    Topic5  0.049770         battle  1.060789   0.4067  -7.6655\n",
       "2052   Topic5  0.048007           sink  1.165447   0.2765  -7.7016\n",
       "1861   Topic5  0.047302           roam  1.115100   0.3059  -7.7164\n",
       "983    Topic5  0.047308          grown  1.161207   0.2655  -7.7162\n",
       "1004   Topic5  0.046176           hall  1.121337   0.2762  -7.7405\n",
       "622    Topic5  0.045638        doorway  1.246517   0.1586  -7.7522\n",
       "\n",
       "[237 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "3         1  1.155423     accept\n",
       "8         1  0.797003        act\n",
       "78        1  1.022067       away\n",
       "81        1  0.951974       baby\n",
       "112       1  0.942694     battle\n",
       "184       1  0.647053      block\n",
       "231       1  1.047239      break\n",
       "411       1  0.985142       come\n",
       "415       1  1.055979    command\n",
       "493       1  1.120430  criticize\n",
       "527       1  1.189903     danced\n",
       "535       1  1.050378       dark\n",
       "544       1  0.822096       dawn\n",
       "545       1  0.962182        day\n",
       "550       1  0.935920       dear\n",
       "586       1  1.278128        dew\n",
       "621       1  0.922200       door\n",
       "622       1  0.802236    doorway\n",
       "647       1  1.148833    driftin\n",
       "654       1  1.361178     driver\n",
       "719       1  0.977793  everybody\n",
       "729       1  0.965169        eye\n",
       "875       1  1.018797     friend\n",
       "944       1  0.914454       gold\n",
       "947       1  0.985621        gon\n",
       "948       1  1.001257       gone\n",
       "949       1  0.993970       good\n",
       "953       1  1.000673        got\n",
       "983       1  0.861173      grown\n",
       "1002      1  0.963923       hair\n",
       "...     ...       ...        ...\n",
       "1413      1  1.115153       meat\n",
       "1433      1  0.661383     mighty\n",
       "1437      1  0.953127       mind\n",
       "1472      1  0.858004     mother\n",
       "1523      1  0.950017      night\n",
       "1535      1  1.040801       note\n",
       "1553      1  0.980172        old\n",
       "1624      1  1.299985        pen\n",
       "1712      1  1.058878     pretty\n",
       "1761      4  1.168213      quinn\n",
       "1799      1  0.828222      ready\n",
       "1837      1  1.348988     reward\n",
       "1847      1  0.949385      right\n",
       "1861      1  0.896780       roam\n",
       "1904      1  0.990829       said\n",
       "1928      1  0.997598        say\n",
       "1976      1  1.000085      seven\n",
       "2011      1  0.911455       ship\n",
       "2019      1  1.021728      shore\n",
       "2052      1  0.858040       sink\n",
       "2125      1  0.920510     speaks\n",
       "2187      1  0.928351      stone\n",
       "2300      1  0.989030       tell\n",
       "2320      1  0.938677      thing\n",
       "2328      1  0.933053   thousand\n",
       "2345      1  0.969895       time\n",
       "2481      1  1.076713        wan\n",
       "2485      1  0.983843       want\n",
       "2506      1  1.006628        way\n",
       "2524      1  0.970883    weekend\n",
       "\n",
       "[86 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 5, 3, 4])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, X, tfidf, mds='tsne')\n",
    "pyLDAvis.save_html(panel, 'lda.html')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NMF vs. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) are both topic modelling tools. The main difference is that LDA takes a Bayesian approach and adds a Dirichlet prior on top of the generative model. NMF’s topic-word probability distributions are fixed, while LDA’s topic-word distributions vary based on how the prior was tuned (hyperparameter $k$ - number of components). NMF would be a better choice if the topic probabilities are fixed for each document ref. Also, if our dataset is small, LDA may have inferior performance since it could introduce too much variability to the model ref.\n",
    "\n",
    "Unlike NMF, reconstructing X with LDA is not a closed-form solution. We need to use Monte Carlo simulations to sample from the distribution of Z (the distribution of topics for each sample), followed by the distribution of W (the distribution of words for topic $Z_i$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
